{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contents:**\n",
    "* [Tokenizing Words and Sentences](#0)\n",
    "* [Stemming using NLTK](#1)\n",
    "* [Lemmatization using NLTK](#2)\n",
    "* [Download stopwords](#3)\n",
    "* [Stop word removal using NLTK](#4)\n",
    "* [Parts of Speech Tagging](#5)\n",
    "* [Named Entity Recognition](#6)\n",
    "* [Text Modelling using Bag of Words Modelling](#7)\n",
    "* [Text MOdeling using TF-IDF Model](#8)\n",
    "* [N-gram model](#9)\n",
    "* [Building Character N-gram model: n=6](#10)\n",
    "* [Building Word N-Gram Model](#11)\n",
    "* [Latent Semantic Analysis](#12)\n",
    "* [Word Synonyms and Antonyms using NLTK](#13)\n",
    "* [Word Negation Tracking in Python: not_happy](#14)\n",
    "* [Word Negation Tracking in Python: unhappy](#15)\n",
    "* [Text Classification](#16)\n",
    "* [Twitter Sentiment Anaysis](#17)\n",
    "* [Creating an Artile Summarizer](#18)\n",
    "* [Word2Vec](#19)\n",
    "* [](#20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Classification:**\n",
    "* [Getting the data for Text Classification](#16.0)\n",
    "* [Preprocessing the data](#16.1)\n",
    "* [Pickling & Unpickling](#16.2)\n",
    "* [Preprocessing the data](#16.3)\n",
    "* [Bag of words model](#16.4)\n",
    "* [Transform BOW model into TF-IDF Model](#16.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Twitter Sentiment Anaysis:**\n",
    "* [Initializing Tokens](#17.0)\n",
    "* [Client Authentication](#17.1)\n",
    "* [Fetching real time tweets](#17.2)\n",
    "* [Loading TF-IDF Model and Classifier](#17.3)\n",
    "* [Preprocessing the tweets](#17.4)\n",
    "* [Plotting the results](#17.5)\n",
    "* [Plotting the bar chart](#17.6)\n",
    "* [Passwords](#17.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating an Artile Summarizer:**\n",
    "* [Text Summarization-Techniques](#18.0)\n",
    "* [Fetching article data from the web](#18.1)\n",
    "* [Parsing the data using Beautiful Soup](#18.2)\n",
    "* [tokenizing articles into sentences](#18.3)\n",
    "* [building the histogram](#18.4)\n",
    "* [Getting the summary](#18.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec:**\n",
    "* [ Importing the data](#19.0)\n",
    "* [Preprocessing the data](#19.1)\n",
    "* [Preparing the data](#19.2)\n",
    "* [Training the Word2Vec Model](#19.3)\n",
    "* [Testing model performance](#19.4)\n",
    "* [Improving Performance](#19.5)\n",
    "* [Exploring Pre-trained models](#19.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "## Tokenizing Words and Sentences\n",
    "\n",
    "Tokenization is a process in which a sequence is broken down into pieces such as words, sentences, phrases etc.\n",
    "\n",
    "Word Tokenization:\n",
    "\n",
    "In this process a sequence like a sentence or a paragraph is broken down into words. These tokenizations are carried out based on the delimiter \"space\" (\"  \").\n",
    "\n",
    "Sentence Tokenization:\n",
    "\n",
    "In this process instead of tokenizing a paragraph based on \"space\", we tokenize it based on \".\" and \",\". Therefore, we get all the different sentences consisting the paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Thank you all so very much. Thank you to the Academy. \n",
    "               Thank you to all of you in this room. I have to congratulate \n",
    "               the other incredible nominees this year. The Revenant was \n",
    "               the product of the tireless efforts of an unbelievable cast\n",
    "               and crew. First off, to my brother in this endeavor, Mr. Tom \n",
    "               Hardy. Tom, your talent on screen can only be surpassed by \n",
    "               your friendship off screen … thank you for creating a t\n",
    "               ranscendent cinematic experience. Thank you to everybody at \n",
    "               Fox and New Regency … my entire team. I have to thank \n",
    "               everyone from the very onset of my career … To my parents; \n",
    "               none of this would be possible without you. And to my \n",
    "               friends, I love you dearly; you know who you are. And lastly,\n",
    "               I just want to say this: Making The Revenant was about\n",
    "               man's relationship to the natural world. A world that we\n",
    "               collectively felt in 2015 as the hottest year in recorded\n",
    "               history. Our production needed to move to the southern\n",
    "               tip of this planet just to be able to find snow. Climate\n",
    "               change is real, it is happening right now. It is the most\n",
    "               urgent threat facing our entire species, and we need to work\n",
    "               collectively together and stop procrastinating. We need to\n",
    "               support leaders around the world who do not speak for the \n",
    "               big polluters, but who speak for all of humanity, for the\n",
    "               indigenous people of the world, for the billions and \n",
    "               billions of underprivileged people out there who would be\n",
    "               most affected by this. For our children’s children, and \n",
    "               for those people out there whose voices have been drowned\n",
    "               out by the politics of greed. I thank you all for this \n",
    "               amazing award tonight. Let us not take this planet for \n",
    "               granted. I do not take tonight for granted. Thank you so very much.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#divide text into sentences\n",
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#divides the text into words\n",
    "words = nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "#word tokenization without NLTK\n",
    "str = \"I love NLP\"\n",
    "words = str.split(\" \")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thank you all so very much', ' Thank you to the Academy', ' \\n               Thank you to all of you in this room', ' I have to congratulate \\n               the other incredible nominees this year', ' The Revenant was \\n               the product of the tireless efforts of an unbelievable cast\\n               and crew', ' First off, to my brother in this endeavor, Mr', ' Tom \\n               Hardy', ' Tom, your talent on screen can only be surpassed by \\n               your friendship off screen … thank you for creating a t\\n               ranscendent cinematic experience', ' Thank you to everybody at \\n               Fox and New Regency … my entire team', ' I have to thank \\n               everyone from the very onset of my career … To my parents; \\n               none of this would be possible without you', ' And to my \\n               friends, I love you dearly; you know who you are', \" And lastly,\\n               I just want to say this: Making The Revenant was about\\n               man's relationship to the natural world\", ' A world that we\\n               collectively felt in 2015 as the hottest year in recorded\\n               history', ' Our production needed to move to the southern\\n               tip of this planet just to be able to find snow', ' Climate\\n               change is real, it is happening right now', ' It is the most\\n               urgent threat facing our entire species, and we need to work\\n               collectively together and stop procrastinating', ' We need to\\n               support leaders around the world who do not speak for the \\n               big polluters, but who speak for all of humanity, for the\\n               indigenous people of the world, for the billions and \\n               billions of underprivileged people out there who would be\\n               most affected by this', ' For our children’s children, and \\n               for those people out there whose voices have been drowned\\n               out by the politics of greed', ' I thank you all for this \\n               amazing award tonight', ' Let us not take this planet for \\n               granted', ' I do not take tonight for granted', ' Thank you so very much', '']\n"
     ]
    }
   ],
   "source": [
    "#For sentence tokenization just replace str.split(\" \")  with str.split(\".\")\n",
    "words = paragraph.split(\".\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## Stemming using NLTK\n",
    "\n",
    "Stemming is the process of reducing infected or derived words to their word stem, base, or root form.\n",
    "\n",
    "Problem: Produced intermediate representation of the word may not have any meaning. Example: intelligen, fina\n",
    "\n",
    "Stemming\n",
    "-Word representation may not have any meaning\n",
    "-Takes less time\n",
    "-Use stemming when meaning of words is not important. Example: Spam detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank you all so very much .',\n",
       " 'Thank you to the Academy .',\n",
       " 'Thank you to all of you in this room .',\n",
       " 'I have to congratulate the other incredible nominees this year .',\n",
       " 'The Revenant was the product of the tireless efforts of an unbelievable cast and crew .',\n",
       " 'First off , to my brother in this endeavor , Mr. Tom Hardy .',\n",
       " 'Tom , your talent on screen can only be surpassed by your friendship off screen … thank you for creating a t ranscendent cinematic experience .',\n",
       " 'Thank you to everybody at Fox and New Regency … my entire team .',\n",
       " 'I have to thank everyone from the very onset of my career … To my parents ; none of this would be possible without you .',\n",
       " 'And to my friends , I love you dearly ; you know who you are .',\n",
       " \"And lastly , I just want to say this : Making The Revenant was about man 's relationship to the natural world .\",\n",
       " 'A world that we collectively felt in 2015 as the hottest year in recorded history .',\n",
       " 'Our production needed to move to the southern tip of this planet just to be able to find snow .',\n",
       " 'Climate change is real , it is happening right now .',\n",
       " 'It is the most urgent threat facing our entire species , and we need to work collectively together and stop procrastinating .',\n",
       " 'We need to support leaders around the world who do not speak for the big polluters , but who speak for all of humanity , for the indigenous people of the world , for the billions and billions of underprivileged people out there who would be most affected by this .',\n",
       " 'For our children ’ s children , and for those people out there whose voices have been drowned out by the politics of greed .',\n",
       " 'I thank you all for this amazing award tonight .',\n",
       " 'Let us not take this planet for granted .',\n",
       " 'I do not take tonight for granted .',\n",
       " 'Thank you so very much .']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#stemming\n",
    "for i in range(len(sentences)):\n",
    "    newwords = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    sentences[i] = \" \".join(newwords)\n",
    "\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## Lemmatization using NLTK\n",
    "\n",
    "Lemmatization: Same as stemming but intermediate representation/root form has a meaning\n",
    "\n",
    "Lemmatization:\n",
    "-word representation have meaning\n",
    "-takes more time than stemming\n",
    "-use lemmatization when meaning of words is important for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank you all so very much .',\n",
       " 'Thank you to the Academy .',\n",
       " 'Thank you to all of you in this room .',\n",
       " 'I have to congratulate the other incredible nominee this year .',\n",
       " 'The Revenant wa the product of the tireless effort of an unbelievable cast and crew .',\n",
       " 'First off , to my brother in this endeavor , Mr. Tom Hardy .',\n",
       " 'Tom , your talent on screen can only be surpassed by your friendship off screen … thank you for creating a t ranscendent cinematic experience .',\n",
       " 'Thank you to everybody at Fox and New Regency … my entire team .',\n",
       " 'I have to thank everyone from the very onset of my career … To my parent ; none of this would be possible without you .',\n",
       " 'And to my friend , I love you dearly ; you know who you are .',\n",
       " \"And lastly , I just want to say this : Making The Revenant wa about man 's relationship to the natural world .\",\n",
       " 'A world that we collectively felt in 2015 a the hottest year in recorded history .',\n",
       " 'Our production needed to move to the southern tip of this planet just to be able to find snow .',\n",
       " 'Climate change is real , it is happening right now .',\n",
       " 'It is the most urgent threat facing our entire specie , and we need to work collectively together and stop procrastinating .',\n",
       " 'We need to support leader around the world who do not speak for the big polluter , but who speak for all of humanity , for the indigenous people of the world , for the billion and billion of underprivileged people out there who would be most affected by this .',\n",
       " 'For our child ’ s child , and for those people out there whose voice have been drowned out by the politics of greed .',\n",
       " 'I thank you all for this amazing award tonight .',\n",
       " 'Let u not take this planet for granted .',\n",
       " 'I do not take tonight for granted .',\n",
       " 'Thank you so very much .']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Lemmatization\t\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    newwords = [lemmatizer.lemmatize(word) for word in words]\n",
    "    sentences[i] = ' '.join(newwords)\n",
    "    \n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "## Download stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Y\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "## Stop word removal using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    newwords = [word for word in words if word not in stopwords.words('english')]\n",
    "    sentences[i] = ' '.join(newwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here are the meanings of the Parts-Of-Speech tags used in NLTK\n",
    "\n",
    "CC\n",
    "\n",
    "Coordinating conjunction\n",
    "\n",
    "CD\n",
    "\n",
    "Cardinal number\n",
    "\n",
    "DT\n",
    "\n",
    "Determiner\n",
    "\n",
    "EX\n",
    "\n",
    "Existential there\n",
    "\n",
    "FW\n",
    "\n",
    "Foreign word\n",
    "\n",
    "IN\n",
    "\n",
    "Preposition or subordinating conjunction\n",
    "\n",
    "JJ\n",
    "\n",
    "Adjective\n",
    "\n",
    "JJR\n",
    "\n",
    "Adjective, comparative\n",
    "\n",
    "JJS\n",
    "\n",
    "Adjective, superlative\n",
    "\n",
    "LS\n",
    "\n",
    "List item marker\n",
    "\n",
    "MD\n",
    "\n",
    "Modal\n",
    "\n",
    "NN\n",
    "\n",
    "Noun, singular or mass\n",
    "\n",
    "NNS\n",
    "\n",
    "Noun, plural\n",
    "\n",
    "NNP\n",
    "\n",
    "Proper noun, singular\n",
    "\n",
    "NNPS\n",
    "\n",
    "Proper noun, plural\n",
    "\n",
    "PDT\n",
    "\n",
    "Predeterminer\n",
    "\n",
    "POS\n",
    "\n",
    "Possessive ending\n",
    "\n",
    "PRP\n",
    "\n",
    "Personal pronoun\n",
    "\n",
    "PRP$\n",
    "\n",
    "Possessive pronoun\n",
    "\n",
    "RB\n",
    "\n",
    "Adverb\n",
    "\n",
    "RBR\n",
    "\n",
    "Adverb, comparative\n",
    "\n",
    "RBS\n",
    "\n",
    "Adverb, superlative\n",
    "\n",
    "RP\n",
    "\n",
    "Particle\n",
    "\n",
    "SYM\n",
    "\n",
    "Symbol\n",
    "\n",
    "TO\n",
    "\n",
    "to\n",
    "\n",
    "UH\n",
    "\n",
    "Interjection\n",
    "\n",
    "VB\n",
    "\n",
    "Verb, base form\n",
    "\n",
    "VBD\n",
    "\n",
    "Verb, past tense\n",
    "\n",
    "VBG\n",
    "\n",
    "Verb, gerund or present participle\n",
    "\n",
    "VBN\n",
    "\n",
    "Verb, past participle\n",
    "\n",
    "VBP\n",
    "\n",
    "Verb, non-3rd person singular present\n",
    "\n",
    "VBZ\n",
    "\n",
    "Verb, 3rd person singular present\n",
    "\n",
    "WDT\n",
    "\n",
    "Wh-determiner\n",
    "\n",
    "WP\n",
    "\n",
    "Wh-pronoun\n",
    "\n",
    "WP$\n",
    "\n",
    "Possessive wh-pronoun\n",
    "\n",
    "WRB\n",
    "\n",
    "Wh-adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(paragraph)\n",
    "\n",
    "tagged_words = nltk.pos_tag(words)\n",
    "\n",
    "word_tags = []\n",
    "\n",
    "for tw in tagged_words:\n",
    "    word_tags.append(tw[0]+\"_\"+tw[0])\n",
    "\n",
    "tagged_paragraph = ' '.join(word_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6'></a>\n",
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paragraph = \"\"\"The Taj Mahal was built by Emperor Shah Jahan\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(paragraph) #tokenize into words\n",
    "tagged_words = nltk.pos_tag(words) #tag the words with parts of speech\n",
    "namedEnt = nltk.ne_chunk(tagged_words) #named entity recognition\n",
    "namedEnt.draw() #draw tree diagram of paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "ORGANIZATION\tGeorgia-Pacific Corp., WHO\n",
    "PERSON\tEddy Bonte, President Obama\n",
    "LOCATION\tMurray River, Mount Everest\n",
    "DATE\tJune, 2008-06-29\n",
    "TIME\ttwo fifty a m, 1:30 p.m.\n",
    "MONEY\t175 million Canadian Dollars, GBP 10.40\n",
    "PERCENT\ttwenty pct, 18.75 %\n",
    "FACILITY\tWashington Monument, Stonehenge\n",
    "GPE\tSouth East Asia, Midlothian\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "## Text Modelling using Bag of Words Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cleaning up the text\n",
    "dataset = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i] = dataset[i].lower() #make all text lowercase\n",
    "    dataset[i] = re.sub(r'\\W',' ', dataset[i]) #remove punctuation\n",
    "    dataset[i] = re.sub(r'\\s+','', dataset[i]) #get rid of spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating the histogram\n",
    "word2count = {}\n",
    "for data in dataset:\n",
    "    words = nltk.word_tokenize(data)\n",
    "    for word in words:\n",
    "        if word not in word2count.keys():\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#finding the most frequent words\n",
    "import heapq\n",
    "\n",
    "freq_words = heapq.nlargest(100, word2count, key=word2count.get) #if using huge corpus then use 3000-5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#final step in creating a bag of word model\n",
    "X = []\n",
    "for data in dataset:\n",
    "    vector = []\n",
    "    for word in freq_words:\n",
    "        if word in nltk.word_tokenize(data):\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    X.append(vector)\n",
    "\n",
    "X = np.asarray(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8'></a>\n",
    "## Text MOdeling using TF-IDF Model\n",
    "\n",
    "#### Bag of Words -Problems:\n",
    "##### -all words have the same importance\n",
    "##### -no semantic information preserved\n",
    "\n",
    "#### Solution: TF-IDF Model:\n",
    "##### -some semantic information is preserved as uncommon words are given more importance than common words\n",
    "##### TF = term frequency: freq of particular word in a particular document\n",
    "##### IDF = inverse document frequency of word in the whole corpus\n",
    "##### TF-IDF = TF * IDF\n",
    "##### TF Formula = (Number of occurrences of a word in a document)/ (Number of words in that document)\n",
    "##### IDF Formula = log[(Number of documents)/(Number of documents containing word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i] = dataset[i].lower()\n",
    "    dataset[i] = re.sub(r'\\W',' ', dataset[i]) #remove punctuation\n",
    "    dataset[i] = re.sub(r'\\s+','', dataset[i]) #get rid of spaces\n",
    "    \n",
    "#Creating the histogram\n",
    "word2count = {}\n",
    "for data in dataset:\n",
    "    words = nltk.word_tokenize(data)\n",
    "    for word in words:\n",
    "        if word not in word2count.keys():\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] +=1\n",
    "            \n",
    "#finding the most frequent words\n",
    "import heapq\n",
    "\n",
    "freq_words = heapq.nlargest(100, word2count, key=word2count.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#IDF Matrix\n",
    "\n",
    "word_idfs = {}\n",
    "\n",
    "for word in freq_words:\n",
    "    doc_count = 0\n",
    "    for data in dataset:\n",
    "        if word in nltk.word_tokenize(data):\n",
    "            doc_count += 1\n",
    "    word_idfs[word] = np.log((len(dataset)/doc_count)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TF Matrix\n",
    "\n",
    "tf_matrix ={}\n",
    "for word in freq_words: #loop through all frequent words\n",
    "    doc_tf = [] #initialize list, for each word, have a vector, contain TF value for all words in the document\n",
    "    for data in dataset: #loop through different sentences\n",
    "        frequency = 0 #for each tf, intialize freq=0\n",
    "        for w in nltk.word_tokenize(data): #for each word in the specific document, if the word == our frequent word or not\n",
    "            if w == word: #checking if word in particular document = to our word\n",
    "                frequency += 1 #freq of each word in each document\n",
    "        tf_word = frequency / len(nltk.word_tokenize(data)) #freq of word in doc/ total # of words in doc\n",
    "        doc_tf.append(tf_word)\n",
    "    tf_matrix[word] = doc_tf #matrix of keys=words & values=list of total # of docs tf's (i.e 21 docs, 21 values)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TF-IDF Calculation\n",
    "\n",
    "tfidf_matrix = []\n",
    "for word in tf_matrix.keys(): #loop through TF matrix\n",
    "    tfidf = [] #will contain all TF-IDF scores for different words, empty list\n",
    "    for value in tf_matrix[word]: #looping though the list (i.e.21 docs)\n",
    "        score = value * word_idfs[word] #value=term freq for a specific word in a specific document\n",
    "        tfidf.append(score)\n",
    "    tfidf_matrix.append(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert tfidf_matrix to array & transpose\n",
    "X = np.asarray(tfidf_matrix) #convert tfidf matrix to 2-dimensional array\n",
    "\n",
    "x = np.transpose(X) #get columns as features/words, rows different documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='9'></a>\n",
    "## N-gram model\n",
    "\n",
    "### Markov Chains-chain of states\n",
    "#### -A is a state\n",
    "#### -B is a state\n",
    "\n",
    "#### -Prob of A->B: 50%\n",
    "#### -Prob of A->A: 50%\n",
    "#### -Prob of B->A: 50%\n",
    "#### -Prob of B->B: 50%\n",
    "\n",
    "### chain of states/markov chain: AABA\n",
    "\n",
    "#### -An N-gram is a contiguous sequence of n items from a given sample of text or speech\n",
    "\n",
    "### Characters/words are the statese of Markov Chains-chain\n",
    "\n",
    "#### \"the bird is flying on the blue sky\"\n",
    "\n",
    "#### N=2\n",
    "\n",
    "#### Bigrams = 'th','he','e ',' b','bi','ir','rd','d',' i' etc... \n",
    "\n",
    "#### Trigrams:\tNext\n",
    "#### 'the' ->\t\t' '\n",
    "#### 'he ' -> \t\t'b'\n",
    "#### 'e b' ->\t\t'i'\n",
    "\n",
    "#### Trigrams:\t\t\t\t\tNext:(can appear in other sentences)\n",
    "#### 'the bird is' \t\t->\t\t['flying','eating','sleeping'] \n",
    "#### 'bird is flyinig' \t->\t\t['on','through','on']\n",
    "#### 'is flying on'\t\t->\t\t['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#N-Gram Modeling-Character Grams\n",
    "#import libraries\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample data\n",
    "text = \"\"\"Global warming or climate change has become a worldwide concern. It is gradually developing into an unprecedented environmental crisis evident in melting glaciers, changing weather patterns, rising sea levels, floods, cyclones and droughts. Global warming implies an increase in the average temperature of the Earth due to entrapment of greenhouse gases in the earth’s atmosphere.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=3 #tri-grams\n",
    "\n",
    "ngrams = {}\n",
    "\n",
    "#Creates the n-grams\n",
    "for i in range(len(text)-n): #-n cause doing n-grams\n",
    "    gram = text[i:i+n] #text[0:3]=Glo\n",
    "    if gram not in ngrams.keys():\n",
    "        ngrams[gram] = [] #list specific to an n-gram\n",
    "    ngrams[gram].append(text[i+n]) #text[0+3] = text[3] = b\n",
    "    \n",
    "ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#text = \"\"\"Global warming or climate change has become a worldwide concern. It is gradually developing into an unprecedented \n",
    "#environmental crisis evident in melting glaciers, changing weather patterns, rising sea levels, floods, cyclones and droughts. \n",
    "#Global warming implies an increase in the average temperature of the Earth due to entrapment of greenhouse gases in the earth’s \n",
    "#atmosphere.\"\"\"\n",
    "\n",
    "# Testing our N-Gram model\n",
    "currentGram = text[0:n]\n",
    "result = currentGram\n",
    "for i in range(100):\n",
    "    if currentGram not in ngrams.keys():\n",
    "        break #if does not find keys then break because that is the end\n",
    "    possibilities = ngrams[currentGram]\n",
    "    nextItem = possibilities[random.randrange(len(possibilities))] #if rangrange(len(possibilities)) is 5, then random.randrange will return a number for 0 to 5; use as index\n",
    "    result += nextItem\n",
    "    currentGram = result[len(result)-n:len(result)] #new current gram are the last 3 results from this char stream as our new current gram\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='10'></a>\n",
    "## Making it better: building Character N-gram model: n=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#text = \"\"\"Global warming or climate change has become a worldwide concern. It is gradually developing into an unprecedented \n",
    "#environmental crisis evident in melting glaciers, changing weather patterns, rising sea levels, floods, cyclones and droughts. \n",
    "#Global warming implies an increase in the average temperature of the Earth due to entrapment of greenhouse gases in the earth’s \n",
    "#atmosphere.\"\"\"\n",
    "\n",
    "n=6\n",
    "\n",
    "ngrams = {}\n",
    "\n",
    "#Creates the n-grams\n",
    "for i in range(len(text)-n):\n",
    "    gram = text[i:i+n] #text[0:3]=Glo\n",
    "    if gram not in ngrams.keys():\n",
    "        ngrams[gram] = []\n",
    "    ngrams[gram].append(text[i+n]) #text[0+3] = text[3] = b\n",
    "    \n",
    "# Testing our N-Gram model\n",
    "currentGram = text[0:n]\n",
    "result = currentGram\n",
    "for i in range(100):\n",
    "    if currentGram not in ngrams.keys():\n",
    "        break\n",
    "    possibilities = ngrams[currentGram]\n",
    "    nextItem = possibilities[random.randrange(len(possibilities))]\n",
    "    result += nextItem\n",
    "    currentGram = result[len(result)-n:len(result)]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='11'></a>\n",
    "## Building Word N-Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Global warming or climate change has become a worldwide concern. It is gradually developing into an unprecedented \n",
    "environmental crisis evident in melting glaciers, changing weather patterns, rising sea levels, floods, cyclones and droughts. \n",
    "Global warming implies an increase in the average temperature of the Earth due to entrapment of greenhouse gases in the earth’s \n",
    "atmosphere.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Order of the grams\n",
    "n = 3\n",
    "\n",
    "# Our N-Grams\n",
    "ngrams = {}\n",
    "\n",
    "# Building the model\n",
    "words = nltk.word_tokenize(text)\n",
    "for i in range(len(words)-n):\n",
    "    gram = ' '.join(words[i:i+n]) #string of n-gram words\n",
    "    if gram not in ngrams.keys():\n",
    "        ngrams[gram] = []\n",
    "    ngrams[gram].append(words[i+n])\n",
    "    \n",
    "# Testing the model\n",
    "currentGram = ' '.join(words[0:n])\n",
    "result = currentGram\n",
    "for i in range(30):\n",
    "    if currentGram not in ngrams.keys():\n",
    "        break\n",
    "    possibilities = ngrams[currentGram]\n",
    "    nextItem = possibilities[random.randrange(len(possibilities))]\n",
    "    result += ' '+nextItem\n",
    "    rWords = nltk.word_tokenize(result)\n",
    "    currentGram = ' '.join(rWords[len(rWords)-n:len(rWords)])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='12'></a>\n",
    "## Latent Semantic Analysis\n",
    "\n",
    "Latent Semantic Analysis is a technique of analysing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.\n",
    "\n",
    "If you have 10 documents, you specify which document is from which concept, i.e. music, tech. \n",
    "\n",
    "Form different concepts: corpus of different documents, and you need to build concepts out of the corpus. For each of the concepts, you will need some key words. These key words can be generated using latent semantic analysis.\n",
    "\n",
    "Topics: Music, Food, News Tech\n",
    "Article: 1,2,3,4,5,6\n",
    "\n",
    "Article 1-85% Music\n",
    "Article 4-100% News\n",
    "Article 6-100% News\n",
    "Article 5-27% News\n",
    "Article 3-100% Food\n",
    "Article 5-73% Food\n",
    "Article 2-100% Tech\n",
    "Article 1-15% Food\n",
    "\n",
    "Singular Value Decomposition Definition:\n",
    "A[mxn] = U[mxr] * S[rxr] * (V[nxr])^T #m*n: m rows, n columns\n",
    "\n",
    "A: Input Data Matrix: m*n matrix (m=number of documents, n=number of words/features)\n",
    "\n",
    "U: Left Singular matrix: m*r matrix (m=number of documents, r= number of concepts)\n",
    "\n",
    "S: Rank Matrix: r*r matrix (r=rank of A)\n",
    "\n",
    "V: Right Singular Matrix: n*r matrix (n=number of words/features, r=number of concepts)\n",
    "\n",
    "Applications: article bucketing in websites, finding relations between articles/words, page indexing in search engines (Google search indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample Data\n",
    "dataset = [\"The amount of polution is increasing day by day\",\n",
    "           \"The concert was just great\",\n",
    "           \"I love to see Gordon Ramsay cook\",\n",
    "           \"Google is introducing a new technology\",\n",
    "           \"AI Robots are examples of great technology present today\",\n",
    "           \"All of us were singing in the concert\",\n",
    "           \"We have launch campaigns to stop pollution and global warming\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 34)\t0.227864387775\n",
      "  (0, 2)\t0.321148397429\n",
      "  (0, 24)\t0.227864387775\n",
      "  (0, 26)\t0.321148397429\n",
      "  (0, 19)\t0.266580749865\n",
      "  (0, 17)\t0.321148397429\n",
      "  (0, 9)\t0.642296794858\n",
      "  (0, 5)\t0.321148397429\n"
     ]
    }
   ],
   "source": [
    "dataset = [line.lower() for line in dataset]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(dataset)\n",
    "\n",
    "print(X[0]) #first number in tuple is which document (0) checking, the second number is the ID for the word (\"The\"), the \"The\"\n",
    "#has a Tfidf of 0.22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=4, n_iter=100,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa = TruncatedSVD(n_components = 4, n_iter=100) #n_components=number of concepts/topic areas, n_iter=number of iterations, higher better could use 300\n",
    "lsa.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.24191973e-01,   1.78240252e-01,   1.14460798e-01,\n",
       "         9.36084432e-17,   1.24191973e-01,   1.14460798e-01,\n",
       "         9.36084432e-17,   3.44988739e-01,   3.74329898e-17,\n",
       "         2.28921595e-01,   1.24191973e-01,   9.36084432e-17,\n",
       "         9.72770950e-02,   3.74329898e-17,   3.00124026e-01,\n",
       "         9.36084432e-17,   1.78240252e-01,   1.14460798e-01,\n",
       "         9.72770950e-02,   1.75760635e-01,   2.37365829e-01,\n",
       "         9.36084432e-17,   3.74329898e-17,   9.72770950e-02,\n",
       "         2.95798061e-01,   9.36084432e-17,   1.14460798e-01,\n",
       "         1.24191973e-01,   3.74329898e-17,   1.24191973e-01,\n",
       "         3.74329898e-17,   1.78240252e-01,   9.36084432e-17,\n",
       "         1.83838346e-01,   3.76098295e-01,   1.08775643e-16,\n",
       "         1.24191973e-01,   1.78240252e-01,   9.36084432e-17,\n",
       "         2.37365829e-01,   9.36084432e-17,   1.78240252e-01])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row1 = lsa.components_[0] #first row is first concept, value for the 42 words for the 1st concept; high value higher prob in concept, low value lower prob not in concept\n",
    "row1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "\n",
      "\n",
      "['ai', 'all', 'amount', 'and', 'are', 'by', 'campaigns', 'concert', 'cook', 'day', 'examples', 'global', 'google', 'gordon', 'great', 'have', 'in', 'increasing', 'introducing', 'is', 'just', 'launch', 'love', 'new', 'of', 'pollution', 'polution', 'present', 'ramsay', 'robots', 'see', 'singing', 'stop', 'technology', 'the', 'to', 'today', 'us', 'warming', 'was', 'we', 'were']\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "print(len(terms))\n",
    "print(\"\\n\")\n",
    "print(terms) #all the terms in the TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concept 0 :\n",
      "('the', 0.37609829529263766)\n",
      "('concert', 0.34498873923306672)\n",
      "('great', 0.30012402589487447)\n",
      "('of', 0.29579806095266653)\n",
      "('just', 0.23736582929791289)\n",
      "('was', 0.23736582929791289)\n",
      "('day', 0.22892159541504406)\n",
      "('technology', 0.18383834567413407)\n",
      "('all', 0.17824025175628985)\n",
      "('in', 0.17824025175628985)\n",
      "\n",
      "Concept 1 :\n",
      "('to', 0.41578844396700693)\n",
      "('cook', 0.2835916579351071)\n",
      "('gordon', 0.2835916579351071)\n",
      "('love', 0.2835916579351071)\n",
      "('ramsay', 0.2835916579351071)\n",
      "('see', 0.2835916579351071)\n",
      "('and', 0.21730644711292471)\n",
      "('campaigns', 0.21730644711292471)\n",
      "('global', 0.21730644711292471)\n",
      "('have', 0.21730644711292471)\n",
      "\n",
      "Concept 2 :\n",
      "('technology', 0.37791806767144054)\n",
      "('is', 0.34196143806319856)\n",
      "('google', 0.34139694419097477)\n",
      "('introducing', 0.34139694419097477)\n",
      "('new', 0.34139694419097477)\n",
      "('day', 0.14112432680994666)\n",
      "('ai', 0.11387892195373087)\n",
      "('are', 0.1138789219537308)\n",
      "('examples', 0.1138789219537308)\n",
      "('present', 0.1138789219537308)\n",
      "\n",
      "Concept 3 :\n",
      "('day', 0.4654267679041128)\n",
      "('amount', 0.23271338395205646)\n",
      "('by', 0.2327133839520564)\n",
      "('increasing', 0.2327133839520564)\n",
      "('polution', 0.2327133839520564)\n",
      "('is', 0.212644552024501)\n",
      "('the', 0.12724213180694352)\n",
      "('all', 0.056446647527264908)\n",
      "('in', 0.05644664752726488)\n",
      "('singing', 0.05644664752726488)\n"
     ]
    }
   ],
   "source": [
    "#top ten key words corresponding to the topics\n",
    "for i, comp in enumerate(lsa.components_):\n",
    "    componentsTerms = zip(terms,comp) #tuple with word and corresponding value of the concept\n",
    "    sortedTerms = sorted(componentsTerms, key=lambda x:x[1], reverse=True) #sort by value x[1] (concept value); words with highest concept value at the top, word with lowest concept value at the bottom of th list\n",
    "    sortedTerms = sortedTerms[:10] #sort the 10 most important terms in a specific concept\n",
    "    print(\"\\nConcept\",i,\":\")\n",
    "    for term in sortedTerms:\n",
    "        print(term)\n",
    "        \n",
    "#Concept 0 is first concept, the most important words are \"the concert great\": from \"music\" concept\n",
    "#Concept 1 is the second concept: from \"food\" concept\n",
    "#Concept 2 is the third concept: from the \"tech\" concept\n",
    "#Concept 3 is the fourth concept: from the \"pollution\" concept\n",
    "#Successfully built a tech cloud for each concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 34)\t0.227864387775\n",
      "  (0, 2)\t0.321148397429\n",
      "  (0, 24)\t0.227864387775\n",
      "  (0, 26)\t0.321148397429\n",
      "  (0, 19)\t0.266580749865\n",
      "  (0, 17)\t0.321148397429\n",
      "  (0, 9)\t0.642296794858\n",
      "  (0, 5)\t0.321148397429\n",
      "[('day', 0.46542676790411119), ('amount', 0.23271338395205565), ('by', 0.2327133839520556), ('increasing', 0.2327133839520556), ('polution', 0.2327133839520556), ('is', 0.21264455202450017), ('the', 0.12724213180694383), ('all', 0.056446647527265539), ('in', 0.056446647527265428), ('singing', 0.056446647527265428)]\n"
     ]
    }
   ],
   "source": [
    "#create a dictionary \n",
    "dataset = [line.lower() for line in dataset]\n",
    "\n",
    "# Creating Tfidf Model\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(dataset)\n",
    "\n",
    "# Visualizing the Tfidf Model\n",
    "print(X[0])\n",
    "\n",
    "\n",
    "# Creating the SVD\n",
    "lsa = TruncatedSVD(n_components = 4, n_iter = 100)\n",
    "lsa.fit(X)\n",
    "\n",
    "\n",
    "# First Column of V\n",
    "row1 = lsa.components_[3]\n",
    "\n",
    "# Word Concept Dictionary Creation\n",
    "concept_words = {}\n",
    "\n",
    "# Visualizing the concepts\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i,comp in enumerate(lsa.components_):\n",
    "    componentTerms = zip(terms,comp)\n",
    "    sortedTerms = sorted(componentTerms,key=lambda x:x[1],reverse=True)\n",
    "    sortedTerms = sortedTerms[:10]\n",
    "    concept_words[i] = sortedTerms\n",
    "    concept_words[\"Concept \"+ str(i)] = sortedTerms\n",
    "print(sortedTerms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0:\n",
      "1.12973954708\n",
      "1.49594271902\n",
      "0\n",
      "0.183838345674\n",
      "0.779760432522\n",
      "1.37336559899\n",
      "0\n",
      "\n",
      "Concept 0:\n",
      "1.12973954708\n",
      "1.49594271902\n",
      "0\n",
      "0.183838345674\n",
      "0.779760432522\n",
      "1.37336559899\n",
      "0\n",
      "\n",
      "1:\n",
      "0\n",
      "0\n",
      "1.83374673364\n",
      "0\n",
      "0\n",
      "0\n",
      "1.28501423242\n",
      "\n",
      "Concept 1:\n",
      "0\n",
      "0\n",
      "1.83374673364\n",
      "0\n",
      "0\n",
      "0\n",
      "1.28501423242\n",
      "\n",
      "2:\n",
      "0.624210091683\n",
      "0\n",
      "0\n",
      "1.74407033831\n",
      "0.833433755486\n",
      "0\n",
      "0\n",
      "\n",
      "Concept 2:\n",
      "0.624210091683\n",
      "0\n",
      "0\n",
      "1.74407033831\n",
      "0.833433755486\n",
      "0\n",
      "0\n",
      "\n",
      "3:\n",
      "2.20159375545\n",
      "0.127242131807\n",
      "0\n",
      "0.212644552025\n",
      "0\n",
      "0.296582074389\n",
      "0\n",
      "\n",
      "Concept 3:\n",
      "2.20159375545\n",
      "0.127242131807\n",
      "0\n",
      "0.212644552025\n",
      "0\n",
      "0.296582074389\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#low amount of data for semantic analysis; would be better with more data/sentences\n",
    "for key in concept_words.keys(): #loop through all the concept & for each concept, we will print out the different sentence scores\n",
    "    sentence_scores = [] #store scores for all the different sentences for the specific concept\n",
    "    for sentence in dataset:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        score = 0\n",
    "        for word in words: #looping through all the words in the sentence\n",
    "            for word_with_score in concept_words[key]: #looping through words in the particular concept; corresponding to each concept word, we will have a list of tuples; looping through list of words corresponding to a concept\n",
    "                if word == word_with_score[0]:\n",
    "                    score += word_with_score[1]\n",
    "        sentence_scores.append(score)\n",
    "    print(\"\\n\"+str(key)+\":\")\n",
    "    for sentence_score in sentence_scores:\n",
    "        print(sentence_score)\n",
    "        \n",
    "#Concept 0 (music) maps to second sentence: \"The concert was just great\" 2nd maps to sixth sentence: \"All of us were singing in the concert\",\n",
    "#Concept 1 (food) maps to third sentence: \"I love to see Gordon Ramsay cook\"; 2nd maps to last sentence: \"We have launch campaigns to stop pollution and global warming\" (wrong)\n",
    "#Concept 2 (tech) maps to fourth sentence: \"Google is introducing a new technology; 2nd maps to 5th sentence: \"AI Robots are examples of great technology present today\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='13'></a>\n",
    "## Word Synonyms and Antonyms using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for s in syn.lemmas(): #loop through synonyms without tags\n",
    "        synonyms.append(s.name())\n",
    "        for a in s.antonyms(): #loop through antonyms\n",
    "            antonyms.append(a.name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'respectable', 'effective', 'soundly', 'serious', 'dependable', 'secure', 'salutary', 'honest', 'safe', 'proficient', 'skillful', 'ripe', 'adept', 'honorable', 'well', 'in_force', 'unspoiled', 'just', 'trade_good', 'upright', 'goodness', 'undecomposed', 'practiced', 'right', 'good', 'dear', 'skilful', 'sound', 'beneficial', 'estimable', 'unspoilt', 'commodity', 'thoroughly', 'near', 'full', 'expert', 'in_effect'}\n",
      "\n",
      "\n",
      "{'ill', 'evil', 'badness', 'evilness', 'bad'}\n"
     ]
    }
   ],
   "source": [
    "print(set(synonyms)) #avoid duplicate synonyms\n",
    "print(\"\\n\")\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='14'></a>\n",
    "## Word Negation Tracking in Python: not_happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"I was not happy with the team's performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'was', 'not', 'happy', 'with', 'the', 'team', \"'s\", 'performance']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.word_tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I was not_happy with the team 's performance\""
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_words = []\n",
    "\n",
    "temp_word = \"\"\n",
    "\n",
    "for word in words:\n",
    "    if word == \"not\":\n",
    "        temp_word = \"not_\"\n",
    "    elif temp_word == \"not_\":\n",
    "        word = temp_word + word #converting not happy to not_happy\n",
    "        temp_word = \"\"\n",
    "    if word != \"not\":\n",
    "        new_words.append(word) #since we have already appended not to the word, i.e. unhappy\n",
    "\n",
    "sentence = ' '.join(new_words)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='15'></a>\n",
    "## Word Negation Tracking in Python: unhappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = \"I was not happy with the team's performance\"\n",
    "\n",
    "words = nltk.word_tokenize(sentence)\n",
    "\n",
    "new_words = []\n",
    "\n",
    "temp_word = ''\n",
    "for word in words:\n",
    "    antonyms = []\n",
    "    if word == 'not':\n",
    "        temp_word = 'not_'\n",
    "    elif temp_word == 'not_':\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for s in syn.lemmas():\n",
    "                for a in s.antonyms():\n",
    "                    antonyms.append(a.name())\n",
    "        if len(antonyms) >= 1:\n",
    "            word = antonyms[0]\n",
    "        else: #if the antonym list is empty, if no antonym exists then use un_word\n",
    "            word = temp_word + word\n",
    "        temp_word = ''\n",
    "    if word != 'not':\n",
    "        new_words.append(word)\n",
    "\n",
    "sentence = ' '.join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I was unhappy with the team 's performance\""
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='16'></a>\n",
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Classification:**\n",
    "* [Getting the data for Text Classification](#16.0)\n",
    "* [Preprocessing the data](#16.1)\n",
    "* [Pickling & Unpickling](#16.2)\n",
    "* [Preprocessing the data](#16.3)\n",
    "* [Bag of words model](#16.4)\n",
    "* [Transform BOW model into TF-IDF Model](#16.5)\n",
    "* [](#16.6)\n",
    "* [](#16.7)\n",
    "* [](#16.8)\n",
    "* [](#16.9)\n",
    "* [](#16.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='16.1'></a>\n",
    "### Getting the data for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Y\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cornell sentiment analysis dataset\n",
    "\n",
    "#Importing the libraries\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing DataSet\n",
    "reviews = load_files('txt_sentoken/') #it will loop through all directories contain in txt_setoken folder, will generate two classes: neg: class 0 & pos: class 1\n",
    "X,y = reviews.data, reviews.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='16.2'></a>\n",
    "### Pickling & Unpickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Storing as Pickle Files\n",
    "with open('X.pickle', 'wb') as f:\n",
    "    pickle.dump(X,f)\n",
    "\n",
    "with open('y.pickle', 'wb') as f:\n",
    "    pickle.dump(y,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Unpickling the dataset\n",
    "\n",
    "with open('X.pickle', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "with open('y.pickle', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='16.3'></a>\n",
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating the corpus\n",
    "corpus = []\n",
    "for i in range(0,len(X)):\n",
    "\treview = re.sub(r'\\W',' ',str(X[i])) #remove non-word characters\n",
    "\treview = review.lower() #change all char to lower case\n",
    "\treview = re.sub(r'\\s+[a-z]\\s+',' ',review)#remove single characters\n",
    "\treview = re.sub(r'^[a-z]\\s+',' ', review) #remove single char in beg of text\n",
    "\treview = re.sub(r'\\s+',' ',review) #remove one or more spaces\n",
    "\tcorpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='16.4'></a>\n",
    "### Bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=2000, min_df = 3, max_df=.6, stop_words = stopwords.words('english'))\n",
    "#max_features: 2000 most frequent words as features excluding rest; min_df = 3: exclude all words who are included in 3 or fewer documents; max_df = .6: 60% exclude all words who appear in 60% or more of the documents, i.e. the, that; stop_words: exclude all stop words in this list of stop words\n",
    "X = vectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='16.5'></a>\n",
    "### Transform BOW model into TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "X = transformer.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='16.6'></a>\n",
    "### Creating a training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_train, text_test, sent_train, sent_test = train_test_split(X,y, test_size=.2, random_state=0)\n",
    "#sent_train = sentiment analysis training, sent_test=sentiment analysis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='16.7'></a>\n",
    "### Understanding Logistic Regression\n",
    "\n",
    "#### -the sentiment analysis task is mainly a binary classification problem to predict whether a given sentence is positive or negative. In our demonstratin we denote '0' as negative and '1' as positive\n",
    "#### -if the point is greater than .5 then positive else negative\n",
    "#### -a learning algorithm is a specific type of algorithm whose performance increases with time. Logistic regression is a type of learning algorith.\n",
    "#### -the algorithm finds the optimal values for the coefficients\n",
    "#### -we need a way to restrict the value of y within range 0 and 1: use exponential fcn: y = e^(equation)/((e^equation) +1)\n",
    "\tln (y/y-1) = equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='16.8'></a>\n",
    "### Training our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "classifier.fit(text_train, sent_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='16.9'></a>\n",
    "### Testing Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_pred = classifier.predict(text_test)\n",
    "cm = confusion_matrix(sent_test, sent_pred)\n",
    "acccuracy = (cm[0][0] + cm[1][1])/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='16.10'></a>\n",
    "### Saving our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Unpickling the dataset\n",
    "\n",
    "with open('X.pickle', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "with open('y.pickle', 'rb') as f:\n",
    "    y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating the corpus\n",
    "corpus = []\n",
    "for i in range(0,len(X)):\n",
    "\treview = re.sub(r'\\W',' ',str(X[i])) #remove non-word characters\n",
    "\treview = review.lower() #change all char to lower case\n",
    "\treview = re.sub(r'\\s+[a-z]\\s+',' ',review)#remove single characters\n",
    "\treview = re.sub(r'^[a-z]\\s+',' ', review) #remove single char in beg of text\n",
    "\treview = re.sub(r'\\s+',' ',review) #remove one or more spaces\n",
    "\tcorpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=2000, min_df = 3, max_df=.6, stop_words = stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pickling the vectorizer\n",
    "with open('tfidfmodel.pickle','wb') as f:\n",
    "\tpickle.dump(vectorizer,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='16.11'></a>\n",
    "## Importing and using our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Unpickling the classifier and vectorizer\n",
    "with open('classifier.pickle', 'rb') as f:\n",
    "\tclf = pickle.load(f) #usual do in a different file\n",
    "\t\n",
    "with open('tfidfmodel.pickle','rb') as f:\n",
    "\ttfidf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "sample = [\"You are a nice person man, have a good life\"]\n",
    "sample = tfidf.transform(sample).toarray()\n",
    "print(clf.predict(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='17'></a>\n",
    "## Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Twitter Sentiment Anaysis:**\n",
    "* [Initializing Tokens](#17.0)\n",
    "* [Client Authentication](#17.1)\n",
    "* [Fetching real time tweets](#17.2)\n",
    "* [Loading TF-IDF Model and Classifier](#17.3)\n",
    "* [Preprocessing the tweets](#17.4)\n",
    "* [Plotting the results](#17.5)\n",
    "* [Plotting the bar chart](#17.6)\n",
    "* [Passwords](#17.7)\n",
    "* [](#17.8)\n",
    "* [](#17.9)\n",
    "* [](#17.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='17.7'></a>\n",
    "jgrosstweets\n",
    "\n",
    "Consumer Key (API Key)\t\n",
    "Consumer Secret (API Secret)\t\n",
    "Access Token\t\n",
    "Access Token Secret\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='17.0'></a>\n",
    "### Initializing Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from tweepy import OAuthHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initializing the keys\n",
    "consumer_key = 'd1yw6mTpin8ym2tiblZe7g5bp'\n",
    "consumer_secret = 'SfsEjyGRll6tVH4Se2SmAsFuG6GcHt30DuREtAfMdSzJyEV6yM'\n",
    "access_token = '963814422354554880-BSGULbmYh9czErQ7bT0K4O9dzVpFbN3'\n",
    "access_secret = 'wpH3irLSq6WrPcum5KUbgvEhibvq6MFPDU2YYSM2PKL3E'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='17.1'></a>\n",
    "### Client Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "args = ['facebook']\n",
    "api = tweepy.API(auth, timeout=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='17.2'></a>\n",
    "### Fetching real time tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_tweets = []\n",
    "query = args[0]\n",
    "if len(args) == 1: \n",
    "\tfor status in tweepy.Cursor(api.search, q=query+\" -filter:retweets\", lang='en', result_type='recent').items(100):\n",
    "\t\tlist_tweets.append(status.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Next Wednesday! https://t.co/sxWBXNAc5J',\n",
       " 'One of Facebook’s most senior engineers just became Director of Engineering, Blockchain - https://t.co/NesBxgXej7 https://t.co/TCLx2urWq3',\n",
       " 'KEEP CALLING YOUR REPRESENTATIVES. IT IS WORKING.\\nSearch Facebook for TOWN HALL. click the link and it will tell yo… https://t.co/0IsU6A9MXQ',\n",
       " 'Somebody in South America just DM, asking what kinds of videos, ads, etc\\n Do I do? I told her...\\nWe make production… https://t.co/DyieE0Ict2',\n",
       " 'Situated perfectly between Dupont and U Street, the location of this spacious one-bedroom is ideal! The shining har… https://t.co/aNTA0y0wyK',\n",
       " 'I added a video to a @YouTube playlist https://t.co/f4q0PfWxL6 Facebook Mass Purge Of Holistic Pages Begins As Doctors Die &amp; Monsanto',\n",
       " 'https://t.co/EmFet9Y5MV\\nMOL this total #Jerk of a cat might be my Thori!!! I couldn´t do any puzzles or something l… https://t.co/ExY8hguFtq',\n",
       " 'Join us tomorrow at 8:45 am at Crestview Walmart for the start up of online pick up service!  We will have a ribbon… https://t.co/NcTYdF6xEP',\n",
       " 'Holy WIND #wind #love #thunder https://t.co/wydpwIFdQk',\n",
       " '**********breaking news********\\n\\nSIPTU votes for strike action at Limerick FC if outstanding wages are not paid\\n\\nSI… https://t.co/4Bp5A7sFuB',\n",
       " 'Agency examining adequacy of disclosures made to investors related to data privacy scandal https://t.co/TxWbPRHJ6c… https://t.co/QwfJoRl7Pm',\n",
       " 'My Grandaughter and I doing a quick HAC💓 https://t.co/9z5Zet0pBw',\n",
       " 'Palm Springs Special Events Grants for the Privilege, Elite and Entitled.\\nSo, we are going to blame it on the Moon… https://t.co/d5Qkpj5z24',\n",
       " 'Our partner in crime @Ellevanmusic invades our Facebook page at 630pm tonight with freestyles, requests, comedy, sh… https://t.co/2TylxW50NU',\n",
       " 'Edinburgh Council Threatens #Free Kids Sports \\n@Edinburgh_CC \\n\\nPlease Read - \\nhttps://t.co/1qhu0hbtL5 https://t.co/9uXq3dOMqB',\n",
       " 'Really....... https://t.co/l6l9gmOZgO',\n",
       " 'Scorpion milking. They normally do spiders, so this may have been a rare opportunity. — at Royal Ontario Museum https://t.co/xAttdmVEZ8',\n",
       " 'Celebrating a safe return. https://t.co/F8EvfVAS24',\n",
       " 'Omg! Obviously I don’t take myself too seriously 😂😂 this is from my business trip to NOLA last year 🙌🏻 https://t.co/bY3NFQFOyG',\n",
       " 'This Thursday at Municipal Park in Churchville ... looking forward to another great show as our amazing summer of 2… https://t.co/vRCJzM7kIh',\n",
       " \"Hi 😈 MidNight  LIVE  'Coz I am Alive...! https://t.co/xdr6eND4YD\",\n",
       " '150 gal mixed reef https://t.co/SVr0HKsjhm',\n",
       " 'How facebook fake accounts affect Election in Pakistan: https://t.co/ixjL9y1i1P via @YouTube',\n",
       " \"21.07.18. Hamburg Exclusive Party on the Ship. Salsa,Kizomba, Zouk, Bachata.Meringue, Reggaeton, Funana..... Let's… https://t.co/dLTEeqpTr6\",\n",
       " '#AfrobeatDashikiParty4\\n{AFROBEAT DASHIKI PARTY 4.0}\\n__________\\nSaturday AUGUST 11th\\nDoors open @ 10pm\\n__________\\nGr… https://t.co/IBBB9xhBPI',\n",
       " \"This is how a leader speaks and acts! \\n\\nWhy so many of the American electorate were so hateful towards this man I'l… https://t.co/qfODGXt6Hg\",\n",
       " 'VIVE Church is hosting a \"Healthy Finances\" course from the Faith in Homebuying series led by Bay Area Realtor, Tam… https://t.co/44UtOfrYXL',\n",
       " 'I so need to get ivy this! https://t.co/MusVBjhPzr',\n",
       " 'Aaron Dixon this weekend but admission fee unless singing/dancing. Too damn hot for me! https://t.co/qAXeuo5eSg',\n",
       " 'Pick your favourite low flyer... https://t.co/1MX23b6p4o',\n",
       " 'So sweet and so funny! https://t.co/5XXqlmTDNU',\n",
       " '#JustListed: HUGE Lot - NO HOA + Casita! \\n\\n📍 Get Price &amp; Location: https://t.co/dQkA7W8rzv\\n\\n#Features: 3 Car Garage… https://t.co/DwvrqPO85J',\n",
       " 'Facebook reminded me that, 3 years ago, @Aarrggghhh and I geeked out to King Diamond and Slayer. It also reminded m… https://t.co/Rtt4owT33x',\n",
       " \"@matthewstoller @blakereid @JustinBrookman @mitchstoltz @sivavaid Absolutely. I'm all for the idea that Facebook, G… https://t.co/KgmBWxNBIN\",\n",
       " 'My lovelies , tomorrow I will be absolutely flat out , I gotta produce a big wedding and I have a lot of prepping t… https://t.co/wG6Acezw0q',\n",
       " '@NahFam__ facebook lol',\n",
       " \"This is Miguel Mondia Elementary School. It's the next destination of the books for the Books Project. https://t.co/nWfOOow38s\",\n",
       " 'I posted a new video to Facebook https://t.co/9fWfzGeZIs',\n",
       " 'When you handpaint a custom watercolor crest featuring your couples monogram and florals from the brides bouquet (… https://t.co/nURou6PsJ1',\n",
       " 'Repost from @tomcruzly using @RepostRegramApp - #oshonobefree ..: this is for my 🎶 for my brothers in Europe..… https://t.co/Q09BIT2yn3',\n",
       " 'Dr. Feingold was an incredible man who helped us so much when Nathan was young. https://t.co/LAOmK5pVuw',\n",
       " \"Look y'all, I'm a life-long anti-racist advocate and activist. I've had a successful career as a musician, horticul… https://t.co/uvCXNtSZaC\",\n",
       " 'I liked a @YouTube video https://t.co/7zPaCK0KWZ Facebook Defends Calls to Shoot Republican Members of Congress',\n",
       " 'Bella loves both https://t.co/O4TThFL5jt',\n",
       " 'Year 6 (Graylings and Monarchs) performing the Pirates of Curry Bean! https://t.co/9cl0MZWdJk',\n",
       " 'MOGS TRUSTEES VIDEO DOWN MEMORY LANE 2018 https://t.co/YohrvqgcAb',\n",
       " 'Will be looking for reference photos for upcoming art 💚\\nIf you have a photo (that you personally took) that can be… https://t.co/J3GKmXv4oe',\n",
       " 'It’s getting hot this summer down here at Carriage House Farm...\\n\\nFilmed at Carriage House Farm by Pirate Koala… https://t.co/41O6L2zWwn',\n",
       " '.\"Sometimes Angels Have Bad Wings.\"  It\\'s about walking away from one relationship and walking right into the same… https://t.co/Sz651YTKgx',\n",
       " 'Laurie Jones Simpson...take heed!  :-D https://t.co/eJe2Juobyx',\n",
       " 'If only this happened here,loads of people have the same disability called stupidity!!! https://t.co/ESheWXPpyV',\n",
       " 'We just posted a photo album from our Burnaby campus student appreciation BBQ. Check it out! https://t.co/JSPQTSoH22',\n",
       " \"2018 Otsuu Cup Women's #Kendo Tournament (宮本武蔵顕彰女子 #剣道大会「#お通杯」)\\n\\nKeikokai: October 13 (Sat), 2018\\nTournament: Octob… https://t.co/0j25u7KGAt\",\n",
       " 'Yep this one is on the list too https://t.co/zGLGgTLL0v',\n",
       " 'When life is really good and when things are really bad , Having meaning gives you something to hold on to! 💯… https://t.co/5WEokzKyaL',\n",
       " 'Facebook was never ephemeral, and now its Stories won’t have to be - https://t.co/mjoTL3ZxGW https://t.co/yhuWCvxyaH',\n",
       " 'Drivers:\\nWe are seeing an issue with our mail and web-server currently, if you are receiving return to sender notic… https://t.co/M8bhamd05V',\n",
       " 'I posted a new video to Facebook https://t.co/iIj7Qfm2O1',\n",
       " 'I got left on read so I deleted my entire Facebook 😂',\n",
       " 'PADI Woman Dive Day 2018 con Sea Spirit Diving in Giardini Naxos... stay Tuned! #gogogo #wearepadi #padi4change… https://t.co/0dfhpSUjLI',\n",
       " 'Obsessed with these new Hottie Lip Plumpers !!😻😻 they go with any look, help reduce wrinkles in your lips and also… https://t.co/qev0tNvMqc',\n",
       " 'Hanoi Jane needs to get a cell next to \"Songbird\"! https://t.co/Z0CMSLiTwR',\n",
       " 'Updates on the Natchez Fire from the Rogue Siskiyou-National Forest. https://t.co/cQTnGEnM41',\n",
       " 'Gotta love it! https://t.co/sse8yYhAso',\n",
       " 'Respect https://t.co/pL9vBVrKb5',\n",
       " 'Police are still looking for witnesses https://t.co/KEQpga4yjr',\n",
       " 'I posted a new video to Facebook https://t.co/JYfHbr6BWW',\n",
       " 'I posted a new video to Facebook https://t.co/NkLBbzwKlh',\n",
       " 'My vision is to \"green\" our parks by using less energy, addressing stormwater, cleaning up our streams and more. Th… https://t.co/r918zQvPh8',\n",
       " 'Always have loads of Facebook memories with Reece in July, and it’s crazy how one year it was about going out drink… https://t.co/7IQbzH3pdg',\n",
       " 'Facebook AI Research Expands With New Academic Collaborations https://t.co/xXJ0gcIa5C https://t.co/QAmoZQD92j',\n",
       " 'I posted a new video to Facebook https://t.co/CS03b6FaEa',\n",
       " 'SUNDAY CITY CRUISE 🚙🚙🚙🚙🚙🚙😍😍😍😍😍😍🇺🇸🇺🇸🇺🇸🇺🇸 #uscarconvention #usccdresden #uscc2018 #sundaycitycruise #dresden #cruise… https://t.co/6YhjontGwr',\n",
       " '@MSNBC oh, but it is happening. Already ran into Russian accounts on both Twitter and Facebook',\n",
       " 'eating pizza at Blaze Pizza https://t.co/8X5lNibfLK',\n",
       " 'Why is @Facebook so soft on #racism? https://t.co/pqetmeO87J',\n",
       " 'There is still time to purchase your tickets through the YMCA link. If you purchase your tickets here, one lucky YM… https://t.co/qwoBSVIiPL',\n",
       " 'It seems that anytime SBS or ABC posts anything negative about Trump on Facebook, the dregs of Aussie society emerg… https://t.co/nDYzif0AJb',\n",
       " \"Photos from Myrella Vazquez's post in Marble Falls Athletic Booster Club https://t.co/AJWoEcGCpW\",\n",
       " 'Manuel and Jacob.. big buddies https://t.co/jyfzNxzHRb',\n",
       " \"Facebook says it's battling fake news, but defends refusal to ban InfoWars https://t.co/URcg7xEs0Q\",\n",
       " \"If you missed our #LunchNLearn with @CoreInsights today, don't despair! Head over to our Facebook events page &amp; sig… https://t.co/YVJvxUJyHL\",\n",
       " 'House Rep suggests converting Google, Facebook, Twitter into public\\xa0utilities https://t.co/Pv0kLBGm8o https://t.co/xbNwoh6R2J',\n",
       " \"Introducing Bondage A Go Go's SummerSlam Wrestling Theme Night Special Guest Referees!\\n\\nTNA/Impact Wrestling Supers… https://t.co/wuzVQTsAXf\",\n",
       " '[Promo vid] Hang out with me, this week on David Kau’s #SouthAfricansLivingInNewYork - tune in for episode 2! Follo… https://t.co/T5iu53uhqN',\n",
       " '@lasagnahog I am always down to join your commune. I assume this is how we will eat there. https://t.co/sZ8DoxZf4D',\n",
       " 'House Rep suggests converting Google, Facebook, Twitter into public\\xa0utilities https://t.co/Twj5OgvElG https://t.co/jrYVHXW7EQ',\n",
       " 'Nice one Indeed. if it could indpire people around. https://t.co/otT8MotQR9',\n",
       " 'More fun being had 40 years ago at The Pretty Things re-union gig - L to R: Phil May, Wally Waller, Dick Taylor, Jo… https://t.co/n69E774rS7',\n",
       " 'Open House By Appointment. \\nWarren Crossing at Warren Twp. NJ Updates\\n35 Luxury Townhome Style Condos For Sale,\\nAva… https://t.co/EKs2ZRJuuq',\n",
       " \"The saga with my son &amp; the #mentalhealthsystem continues. I have an update on FB. I'm grateful for so many people t… https://t.co/uJlkh9EY0f\",\n",
       " 'https://t.co/Cmc2g72SJf Continental Youth Championships Boston MA at The Irish Cultural Center of New England in Ca… https://t.co/P4DRWz4iH0',\n",
       " 'I posted a new video to Facebook https://t.co/753VWSBtxO',\n",
       " 'Slow the breath, slow the flow, maybe slow your roll. Slowing down can be challenging, physically and mentally. I f… https://t.co/9TnCoafgD3',\n",
       " 'Straight facts https://t.co/XH0YWZmDFS',\n",
       " '😂😂😂😂😂😂😂 Awh man I almost choked https://t.co/TphzBFAoRz',\n",
       " 'Teachers, don’t miss this great opportunity for funding for your classrooms! Please share Chloe Vignes Kevin McAuli… https://t.co/r8ZU2XOWh5',\n",
       " 'Mark your calendar! Click \"Going\" to set a reminder! https://t.co/Qfeh7jlYRD',\n",
       " '*WHERE ARE WE GOING NIGERIANS* ❓\\n\\n_Gone are the days when ballot boxes are snatched after elections, these days wha… https://t.co/ltcs52YzWs',\n",
       " \"@Surprisedzoe @BigFashionista @IAmWitWitWoo I deactivated my facebook a while ago so didn';t see it\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='17.3'></a>\n",
    "Loading TF-IDF Model and Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tfidfmodel.pickle', 'rb') as f:\n",
    "\tvectorizer = pickle.load(f)\n",
    "\t\n",
    "with open('classifier.pickle', 'rb') as f:\n",
    "\tclf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(vectorizer.transform(['You are a nice person, have a good life']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='17.4'></a>\n",
    "### Preprocessing the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next wednesday \n",
      "one of facebook s mo senior engineers ju became director of engineering blockchain https t co nesbxgxej \n",
      "keep calling your representatives it is working search facebook for town hall click the link and it will tell yo \n",
      "somebody in south america ju dm asking what kinds of videos ads etc do i do i told her we make production \n",
      "situated perfectly between dupont and u street the location of this spacious one bedroom is ideal the shining har \n",
      "i added a video to a youtube playli https t co f q pfwxl facebook ma purge of holistic pages begins as doctors die amp monsanto\n",
      "https t co emfet y mv mol this total jerk of a cat might be my thori i couldn t do any puzzles or something l \n",
      "join us tomorrow at am at crestview walmart for the start up of online pick up service we will have a ribbon \n",
      "holy wind wind love thunder \n",
      " breaking news siptu votes for strike action at limerick fc if outstanding wages are not paid \n",
      "agency examining adequacy of disclosures made to investors related to data privacy scandal https t co txwbprhj c \n",
      "my grandaughter and i doing a quick hac \n",
      "palm springs special events grants for the privilege elite and entitled we are going to blame it on the moon \n",
      "our partner in crime ellevanmusic invades our facebook page at pm tonight with freestyles requests comedy \n",
      "edinburgh council threatens free kids sports edinburgh_cc plea read https t co qhu hbtl \n",
      "really \n",
      "scorpion milking they normally do spiders this may have been a rare opportunity at royal ontario museum \n",
      "celebrating a safe return \n",
      "omg obviously i don t take myself too seriously this is from my busine trip to nola la year \n",
      "this thursday at municipal park in churchville looking forward to another great show as our amazing summer of \n",
      "hi midnight live coz i am alive \n",
      " gal mixed reef \n",
      "how facebook fake accounts affect election in pakistan https t co ixjl y i p via youtube\n",
      " hamburg exclusive party on the ship sal kizomba zouk bachata meringue reggaeton funana let s \n",
      " afrobeatdashikiparty afrobeat dashiki party __________ saturday augu th doors open pm __________ gr \n",
      "this is how a leader speaks and acts why many of the american electorate were hateful towards this man i l \n",
      "vive church is hosting a healthy finances cour from the faith in homebuying series led by bay area realtor tam \n",
      "i need to get ivy this \n",
      "aaron dixon this weekend but admission fee unle singing dancing too damn hot for me \n",
      "pick your favourite low flyer \n",
      " sweet and funny \n",
      " justlisted huge lot no hoa casita get price amp location https t co dqka w rzv features car garage \n",
      "facebook reminded me that years ago aarrggghhh and i geeked out to king diamond and slayer it al reminded m \n",
      " matthewstoller blakereid justinbrookman mitchstoltz sivavaid absolutely i am all for the idea that facebook g \n",
      "my lovelies tomorrow i will be absolutely flat out i gotta produce a big wedding and i have a lot of prepping t \n",
      " nahfam__ facebook lol\n",
      "this is miguel mondia elementary school it is the next destination of the books for the books project \n",
      "i posted a new video to facebook \n",
      "when you handpaint a custom watercolor cre featuring your couples monogram and florals from the brides bouquet \n",
      "repo from tomcruzly using repostregramapp oshonobefree this is for my for my brothers in europe \n",
      "dr feingold was an incredible man who helped us much when nathan was young \n",
      "look y all i am a life long anti raci advocate and activi i ve had a successful career as a musician horticul \n",
      "i liked a youtube video https t co zpack kwz facebook defends calls to shoot republican members of congre \n",
      "bella loves both \n",
      "year graylings and monarchs performing the pirates of curry bean \n",
      "mogs trustees video down memory lane \n",
      "will be looking for reference photos for upcoming art if you have a photo that you personally took that can be \n",
      "it s getting hot this summer down here at carriage hou farm filmed at carriage hou farm by pirate koala \n",
      " sometimes angels have bad wings it is about walking away from one relationship and walking right into the same \n",
      "laurie jones simpson take heed d \n",
      "if only this happened here loads of people have the same disability called stupidity \n",
      "we ju posted a photo album from our burnaby campus student appreciation bbq check it out \n",
      " otsuu cup women s kendo tournament 宮本武蔵顕彰女子 剣道大会 お通杯 keikokai october sat tournament octob \n",
      "yep this one is on the li too \n",
      "when life is really good and when things are really bad having meaning gives you something to hold on to \n",
      "facebook was never ephemeral and now its stories won t have to be https t co mjotl zxgw \n",
      "drivers we are seeing an issue with our mail and web server currently if you are receiving return to sender notic \n",
      "i posted a new video to facebook \n",
      "i got left on read i deleted my entire facebook \n",
      "padi woman dive day con sea spirit diving in giardini naxos stay tuned gogogo wearepadi padi change \n",
      "obsessed with the new hottie lip plumpers they go with any look help reduce wrinkles in your lips and al \n",
      "hanoi jane needs to get a cell next to songbird \n",
      "updates on the natchez fire from the rogue siskiyou national fore \n",
      "gotta love it \n",
      "respect \n",
      "police are still looking for witnesses \n",
      "i posted a new video to facebook \n",
      "i posted a new video to facebook \n",
      "my vision is to green our parks by using le energy addressing stormwater cleaning up our streams and more th \n",
      "always have loads of facebook memories with reece in july and it s crazy how one year it was about going out drink \n",
      "facebook ai research expands with new academic collaborations https t co xxj gcia c \n",
      "i posted a new video to facebook \n",
      "sunday city crui uscarconvention usccdresden uscc sundaycitycrui dresden crui \n",
      " msnbc oh but it is happening already ran into russian accounts on both twitter and facebook\n",
      "eating pizza at blaze pizza \n",
      "why is facebook soft on raci \n",
      "there is still time to purcha your tickets through the ymca link if you purcha your tickets here one lucky ym \n",
      "it seems that anytime sbs or abc posts anything negative about trump on facebook the dregs of aussie society emerg \n",
      "photos from myrella vazquez s po in marble falls athletic booster club \n",
      "manuel and jacob big buddies \n",
      "facebook says it is battling fake news but defends refusal to ban infowars \n",
      "if you missed our lunchnlearn with coreinsights today don t despair head over to our facebook events page amp sig \n",
      "hou rep suggests converting google facebook twitter into public utilities https t co pv klbgm o \n",
      "introducing bondage a go go s summerslam wrestling theme night special gue referees tna impact wrestling supers \n",
      " promo vid hang out with me this week on david kau s southafricanslivinginnewyork tune in for episode follo \n",
      " lasagnahog i am always down to join your commune i assume this is how we will eat there \n",
      "hou rep suggests converting google facebook twitter into public utilities https t co twj ogvelg \n",
      "nice one indeed if it could indpire people around \n",
      "more fun being had years ago at the pretty things re union gig l to r phil may wally waller dick taylor jo \n",
      "open hou by appointment warren crossing at warren twp nj updates luxury townhome style condos for sale ava \n",
      "the saga with my son amp the mentalhealthsystem continues i have an update on fb i am grateful for many people t \n",
      "https t co cmc g sjf continental youth championships boston ma at the iri cultural center of new england in ca \n",
      "i posted a new video to facebook \n",
      "slow the breath slow the flow maybe slow your roll slowing down can be challenging physically and mentally i f \n",
      "straight facts \n",
      " awh man i almo choked \n",
      "teachers don t mi this great opportunity for funding for your classrooms plea share chloe vignes kevin mcauli \n",
      "mark your calendar click going to set a reminder \n",
      " where are we going nigerians _gone are the days when ballot boxes are snatched after elections the days wha \n",
      " surprisedzoe bigfashionista iamwitwitwoo i deactivated my facebook a while ago didn t see it\n"
     ]
    }
   ],
   "source": [
    "for tweet in list_tweets:\n",
    "\ttweets = re.sub(r\"^https://t.co/[a-zA-Z0-9]*\\s\",\" \",tweet)\n",
    "\ttweet = re.sub(r\"\\s+https//t.co/[a-zA-Z0-9]*\\s\", \" \", tweet)\n",
    "\ttweet = re.sub(r\"\\s+https://t.co/[a-zA-Z0-9]*$\",\" \", tweet)\n",
    "\ttweet = tweet.lower()\n",
    "\ttweet = re.sub(r\"that's\",\"that is\", tweet)\n",
    "\ttweet = re.sub(r\"there's\",\"there is\", tweet)\n",
    "\ttweet = re.sub(r\"what's\",\"what is\", tweet)\n",
    "\ttweet = re.sub(r\"where's\",\"where is\", tweet)\n",
    "\ttweet = re.sub(r\"it's\",\"it is\", tweet)\n",
    "\ttweet = re.sub(r\"who's\",\"who is\", tweet)\n",
    "\ttweet = re.sub(r\"i'm\",\"i am\", tweet)\n",
    "\ttweet = re.sub(r\"she's\",\"she is\", tweet)\n",
    "\ttweet = re.sub(r\"they're\",\"they are\", tweet)\n",
    "\ttweet = re.sub(r\"who're\",\"who are\", tweet)\n",
    "\ttweet = re.sub(r\"ain't\",\"am not\", tweet)\n",
    "\ttweet = re.sub(r\"wouldn't\",\"would not\", tweet)\n",
    "\ttweet = re.sub(r\"shouldn't\",\"should not\", tweet)\n",
    "\ttweet = re.sub(r\"can't\",\"can not\", tweet)\n",
    "\ttweet = re.sub(r\"couldn't\",\"could not\", tweet)\n",
    "\ttweet = re.sub(r\"won't\",\"will not\", tweet)\n",
    "\ttweet = re.sub(r\"\\W\",\" \", tweet) #remove punctuation\n",
    "\ttweet = re.sub(r\"\\d\",\" \", tweet) #digits\n",
    "\ttweet = re.sub(r\"s+[a-z]\\s+\",\" \", tweet) #single characters in middle\n",
    "\ttweet = re.sub(r\"s+[a-z]$\",\" \", tweet) #single characters at end\n",
    "\ttweet = re.sub(r\"^[a-z]$\",\" \", tweet) #single characters at front\n",
    "\ttweet = re.sub(r\"\\s+\",\" \", tweet) #remove all extra spaces\n",
    "\tprint(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='17.4'></a>\n",
    "### Predicting sentiment of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next wednesday  : [1]\n",
      "one of facebook most senior engineers just became director of engineering blockchain  : [1]\n",
      "keep calling your representatives it is working search facebook for town hall click the link and it will tell yo  : [1]\n",
      "somebody in south america just dm asking what kinds of videos ads etc do do told her we make production  : [1]\n",
      "situated perfectly between dupont and street the location of this spacious one bedroom is ideal the shining har  : [1]\n",
      " added video to youtube playlist facebook mass purge of holistic pages begins as doctors die amp monsanto : [0]\n",
      " mol this total jerk of cat might be my thori couldn do any puzzles or something  : [0]\n",
      "join us tomorrow at am at crestview walmart for the start up of online pick up service we will have ribbon  : [0]\n",
      "holy wind wind love thunder  : [1]\n",
      " breaking news siptu votes for strike action at limerick fc if outstanding wages are not paid si  : [1]\n",
      "agency examining adequacy of disclosures made to investors related to data privacy scandal https co txwbprhj  : [0]\n",
      "my grandaughter and doing quick hac  : [1]\n",
      "palm springs special events grants for the privilege elite and entitled so we are going to blame it on the moon  : [0]\n",
      "our partner in crime ellevanmusic invades our facebook page at pm tonight with freestyles requests comedy sh  : [0]\n",
      "edinburgh council threatens free kids sports edinburgh_cc please read  : [0]\n",
      "really  : [1]\n",
      "scorpion milking they normally do spiders so this may have been rare opportunity at royal ontario museum  : [1]\n",
      "celebrating safe return  : [1]\n",
      "omg obviously don take myself too seriously this is from my business trip to nola last year  : [0]\n",
      "this thursday at municipal park in churchville looking forward to another great show as our amazing summer of  : [1]\n",
      "hi midnight live coz am alive  : [1]\n",
      " gal mixed reef  : [1]\n",
      "how facebook fake accounts affect election in pakistan via youtube : [0]\n",
      " hamburg exclusive party on the ship salsa kizomba zouk bachata meringue reggaeton funana let  : [0]\n",
      " afrobeatdashikiparty afrobeat dashiki party __________ saturday august th doors open pm __________ gr  : [1]\n",
      "this is how leader speaks and acts why so many of the american electorate were so hateful towards this man l  : [1]\n",
      "vive church is hosting healthy finances course from the faith in homebuying series led by bay area realtor tam  : [1]\n",
      " so need to get ivy this  : [0]\n",
      "aaron dixon this weekend but admission fee unless singing dancing too damn hot for me  : [0]\n",
      "pick your favourite low flyer  : [0]\n",
      "so sweet and so funny  : [1]\n",
      " justlisted huge lot no hoa casita get price amp location features car garage  : [0]\n",
      "facebook reminded me that years ago aarrggghhh and geeked out to king diamond and slayer it also reminded  : [1]\n",
      " matthewstoller blakereid justinbrookman mitchstoltz sivavaid absolutely am all for the idea that facebook  : [0]\n",
      "my lovelies tomorrow will be absolutely flat out gotta produce big wedding and have lot of prepping  : [0]\n",
      " nahfam__ facebook lol : [1]\n",
      "this is miguel mondia elementary school it is the next destination of the books for the books project  : [1]\n",
      " posted new video to facebook  : [0]\n",
      "when you handpaint custom watercolor crest featuring your couples monogram and florals from the brides bouquet  : [1]\n",
      "repost from tomcruzly using repostregramapp oshonobefree this is for my for my brothers in europe  : [1]\n",
      "dr feingold was an incredible man who helped us so much when nathan was young  : [1]\n",
      "look all am life long anti racist advocate and activist ve had successful career as musician horticul  : [1]\n",
      " liked youtube video facebook defends calls to shoot republican members of congress : [0]\n",
      "bella loves both  : [1]\n",
      "year graylings and monarchs performing the pirates of curry bean  : [1]\n",
      "mogs trustees video down memory lane  : [0]\n",
      "will be looking for reference photos for upcoming art if you have photo that you personally took that can be  : [1]\n",
      "it getting hot this summer down here at carriage house farm filmed at carriage house farm by pirate koala  : [0]\n",
      " sometimes angels have bad wings it is about walking away from one relationship and walking right into the same  : [0]\n",
      "laurie jones simpson take heed  : [0]\n",
      "if only this happened here loads of people have the same disability called stupidity  : [1]\n",
      "we just posted photo album from our burnaby campus student appreciation bbq check it out  : [0]\n",
      " otsuu cup women kendo tournament 宮本武蔵顕彰女子 剣道大会 お通杯 keikokai october sat tournament octob  : [0]\n",
      "yep this one is on the list too  : [0]\n",
      "when life is really good and when things are really bad having meaning gives you something to hold on to  : [1]\n",
      "facebook was never ephemeral and now its stories won have to be  : [1]\n",
      "drivers we are seeing an issue with our mail and web server currently if you are receiving return to sender notic  : [1]\n",
      " posted new video to facebook  : [0]\n",
      " got left on read so deleted my entire facebook  : [0]\n",
      "padi woman dive day con sea spirit diving in giardini naxos stay tuned gogogo wearepadi padi change  : [1]\n",
      "obsessed with these new hottie lip plumpers they go with any look help reduce wrinkles in your lips and also  : [1]\n",
      "hanoi jane needs to get cell next to songbird  : [1]\n",
      "updates on the natchez fire from the rogue siskiyou national forest  : [1]\n",
      "gotta love it  : [1]\n",
      "respect  : [1]\n",
      "police are still looking for witnesses  : [1]\n",
      " posted new video to facebook  : [0]\n",
      " posted new video to facebook  : [0]\n",
      "my vision is to green our parks by using less energy addressing stormwater cleaning up our streams and more th  : [1]\n",
      "always have loads of facebook memories with reece in july and it crazy how one year it was about going out drink  : [1]\n",
      "facebook ai research expands with new academic collaborations  : [1]\n",
      " posted new video to facebook  : [0]\n",
      "sunday city cruise uscarconvention usccdresden uscc sundaycitycruise dresden cruise  : [1]\n",
      " msnbc oh but it is happening already ran into russian accounts on both twitter and facebook : [1]\n",
      "eating pizza at blaze pizza  : [1]\n",
      "why is facebook so soft on racism  : [1]\n",
      "there is still time to purchase your tickets through the ymca link if you purchase your tickets here one lucky ym  : [1]\n",
      "it seems that anytime sbs or abc posts anything negative about trump on facebook the dregs of aussie society emerg  : [0]\n",
      "photos from myrella vazquez post in marble falls athletic booster club  : [0]\n",
      "manuel and jacob big buddies  : [0]\n",
      "facebook says it is battling fake news but defends refusal to ban infowars  : [0]\n",
      "if you missed our lunchnlearn with coreinsights today don despair head over to our facebook events page amp sig  : [1]\n",
      "house rep suggests converting google facebook twitter into public utilities  : [0]\n",
      "introducing bondage go go summerslam wrestling theme night special guest referees tna impact wrestling supers  : [1]\n",
      " promo vid hang out with me this week on david kau southafricanslivinginnewyork tune in for episode follo  : [1]\n",
      " lasagnahog am always down to join your commune assume this is how we will eat there  : [1]\n",
      "house rep suggests converting google facebook twitter into public utilities  : [0]\n",
      "nice one indeed if it could indpire people around  : [1]\n",
      "more fun being had years ago at the pretty things re union gig to phil may wally waller dick taylor jo  : [1]\n",
      "open house by appointment warren crossing at warren twp nj updates luxury townhome style condos for sale ava  : [0]\n",
      "the saga with my son amp the mentalhealthsystem continues have an update on fb am grateful for so many people  : [1]\n",
      " continental youth championships boston ma at the irish cultural center of new england in ca  : [1]\n",
      " posted new video to facebook  : [0]\n",
      "slow the breath slow the flow maybe slow your roll slowing down can be challenging physically and mentally f  : [1]\n",
      "straight facts  : [0]\n",
      " awh man almost choked  : [1]\n",
      "teachers don miss this great opportunity for funding for your classrooms please share chloe vignes kevin mcauli  : [1]\n",
      "mark your calendar click going to set reminder  : [1]\n",
      " where are we going nigerians _gone are the days when ballot boxes are snatched after elections these days wha  : [1]\n",
      " surprisedzoe bigfashionista iamwitwitwoo deactivated my facebook while ago so didn see it : [1]\n"
     ]
    }
   ],
   "source": [
    "for tweet in list_tweets:\n",
    "    tweet = re.sub(r\"^https://t.co/[a-zA-Z0-9]*\\s\", \" \", tweet)\n",
    "    tweet = re.sub(r\"\\s+https://t.co/[a-zA-Z0-9]*\\s\", \" \", tweet)\n",
    "    tweet = re.sub(r\"\\s+https://t.co/[a-zA-Z0-9]*$\", \" \", tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"that's\",\"that is\",tweet)\n",
    "    tweet = re.sub(r\"there's\",\"there is\",tweet)\n",
    "    tweet = re.sub(r\"what's\",\"what is\",tweet)\n",
    "    tweet = re.sub(r\"where's\",\"where is\",tweet)\n",
    "    tweet = re.sub(r\"it's\",\"it is\",tweet)\n",
    "    tweet = re.sub(r\"who's\",\"who is\",tweet)\n",
    "    tweet = re.sub(r\"i'm\",\"i am\",tweet)\n",
    "    tweet = re.sub(r\"she's\",\"she is\",tweet)\n",
    "    tweet = re.sub(r\"he's\",\"he is\",tweet)\n",
    "    tweet = re.sub(r\"they're\",\"they are\",tweet)\n",
    "    tweet = re.sub(r\"who're\",\"who are\",tweet)\n",
    "    tweet = re.sub(r\"ain't\",\"am not\",tweet)\n",
    "    tweet = re.sub(r\"wouldn't\",\"would not\",tweet)\n",
    "    tweet = re.sub(r\"shouldn't\",\"should not\",tweet)\n",
    "    tweet = re.sub(r\"can't\",\"can not\",tweet)\n",
    "    tweet = re.sub(r\"couldn't\",\"could not\",tweet)\n",
    "    tweet = re.sub(r\"won't\",\"will not\",tweet)\n",
    "    tweet = re.sub(r\"\\W\",\" \",tweet) #remove punctuation\n",
    "    tweet = re.sub(r\"\\d\",\" \",tweet) #remove digits\n",
    "    tweet = re.sub(r\"\\s+[a-z]\\s+\",\" \",tweet) #single characters in middle\n",
    "    tweet = re.sub(r\"\\s+[a-z]$\",\" \",tweet) #single characters at end\n",
    "    tweet = re.sub(r\"^[a-z]\\s+\",\" \",tweet) #single characters at front\n",
    "    tweet = re.sub(r\"\\s+\",\" \",tweet) #remove all extra spaces\n",
    "    sent = clf.predict(vectorizer.transform([tweet]).toarray())\n",
    "    print(tweet,\":\",sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='17.5'></a>\n",
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "args = ['google'] #search term\n",
    "api = tweepy.API(auth, timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_tweets = []\n",
    "query = args[0]\n",
    "if len(args) == 1: \n",
    "\tfor status in tweepy.Cursor(api.search, q=query+\" -filter:retweets\", lang='en', result_type='recent').items(100):\n",
    "\t\tlist_tweets.append(status.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_pos = 0\n",
    "total_neg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tweet in list_tweets:\n",
    "    tweet = re.sub(r\"^https://t.co/[a-zA-Z0-9]*\\s\", \" \", tweet)\n",
    "    tweet = re.sub(r\"\\s+https://t.co/[a-zA-Z0-9]*\\s\", \" \", tweet)\n",
    "    tweet = re.sub(r\"\\s+https://t.co/[a-zA-Z0-9]*$\", \" \", tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r\"that's\",\"that is\",tweet)\n",
    "    tweet = re.sub(r\"there's\",\"there is\",tweet)\n",
    "    tweet = re.sub(r\"what's\",\"what is\",tweet)\n",
    "    tweet = re.sub(r\"where's\",\"where is\",tweet)\n",
    "    tweet = re.sub(r\"it's\",\"it is\",tweet)\n",
    "    tweet = re.sub(r\"who's\",\"who is\",tweet)\n",
    "    tweet = re.sub(r\"i'm\",\"i am\",tweet)\n",
    "    tweet = re.sub(r\"she's\",\"she is\",tweet)\n",
    "    tweet = re.sub(r\"he's\",\"he is\",tweet)\n",
    "    tweet = re.sub(r\"they're\",\"they are\",tweet)\n",
    "    tweet = re.sub(r\"who're\",\"who are\",tweet)\n",
    "    tweet = re.sub(r\"ain't\",\"am not\",tweet)\n",
    "    tweet = re.sub(r\"wouldn't\",\"would not\",tweet)\n",
    "    tweet = re.sub(r\"shouldn't\",\"should not\",tweet)\n",
    "    tweet = re.sub(r\"can't\",\"can not\",tweet)\n",
    "    tweet = re.sub(r\"couldn't\",\"could not\",tweet)\n",
    "    tweet = re.sub(r\"won't\",\"will not\",tweet)\n",
    "    tweet = re.sub(r\"\\W\",\" \",tweet) #remove punctuation\n",
    "    tweet = re.sub(r\"\\d\",\" \",tweet) #remove digits\n",
    "    tweet = re.sub(r\"\\s+[a-z]\\s+\",\" \",tweet) #single characters in middle\n",
    "    tweet = re.sub(r\"\\s+[a-z]$\",\" \",tweet) #single characters at end\n",
    "    tweet = re.sub(r\"^[a-z]\\s+\",\" \",tweet) #single characters at front\n",
    "    tweet = re.sub(r\"\\s+\",\" \",tweet) #remove all extra spaces\n",
    "    sent = clf.predict(vectorizer.transform([tweet]).toarray())\n",
    "    if sent[0] == 1: #if pos then incr total_pos\n",
    "        total_pos += 1\n",
    "    else: #if neg then incr total_neg\n",
    "        total_neg += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='17.6'></a>\n",
    "### Plotting the bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEICAYAAABS0fM3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMDPLnIPAzCxzDgIzs8w5CMzMMvdfCyGiwX5HhSEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d9c6a03518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "objects = ['Positive','Negative']\n",
    "y_pos = np.arange(len(objects))\n",
    "\n",
    "plt.bar(y_pos, [total_pos, total_neg], alpha=.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Number')\n",
    "plt.title('Number of Positive and Negative Tweets')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='18'></a>\n",
    "## Creating an Article Summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating an Artile Summarizer:**\n",
    "* [Text Summarization-Techniques](#18.0)\n",
    "* [Fetching article data from the web](#18.1)\n",
    "* [Parsing the data using Beautiful Soup](#18.2)\n",
    "* [tokenizing articles into sentences](#18.3)\n",
    "* [building the histogram](#18.4)\n",
    "* [Getting the summary](#18.5)\n",
    "* [](#18.6)\n",
    "* [](#18.7)\n",
    "* [](#18.8)\n",
    "* [](#18.9)\n",
    "* [](#18.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='18.0'></a>\n",
    "### Text Summarization-Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-a simple natural language processing based approach\n",
    "-a deep NLP based approach\n",
    "\n",
    "1.Tokenization\n",
    "2.PreProcess\n",
    "3.frequency chart\n",
    "4.weighted frequency chart\n",
    "5.sentence scores\n",
    "6.sort sentence scores dictionary desc\n",
    "7.select n-largest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='18.1'></a>\n",
    "### Fetching article data from the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gettings the data source\n",
    "source = urllib.request.urlopen('https://en.wikipedia.org/wiki/Global_warming').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#applying beautiful soup\n",
    "soup = bs.BeautifulSoup(source, \"lxml\") #lxml type of parser that BeautifulSoup using to parse an html document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='18.2'></a>\n",
    "### Parsing the data using Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for paragraph in soup.find_all('p'):\n",
    "    text += paragraph.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing the text\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ',text)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "clean_text = text.lower()\n",
    "clean_text = re.sub(r'\\W',' ',clean_text) #remove non-words, numbers\n",
    "clean_text = re.sub(r'\\d',' ',clean_text) #remove digits\n",
    "clean_text = re.sub(r'\\s+',' ',clean_text) #remove extra spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='18.3'></a>\n",
    "### tokenizing articles into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Y\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "sentences = nltk.sent_tokenize(text) #tokenized paragraph into sentences\n",
    "stop_words = nltk.corpus.stopwords.words('english') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='18.4'></a>\n",
    "### building the histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2count = {}\n",
    "\n",
    "for word in nltk.word_tokenize(clean_text):\n",
    "    if word not in stop_words:\n",
    "        if word not in word2count.keys(): #first occurence of word\n",
    "            word2count[word]= 1\n",
    "        else:\n",
    "            word2count[word] += 1 #subsequent occurence of word          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting counts to weights\n",
    "for key in word2count.keys():\n",
    "    word2count[key] = word2count[key]/max(word2count.values()) #dictionary of words & weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#product sentence scores\n",
    "sent2score = {}\n",
    "for sentence in sentences:\n",
    "    for word in nltk.word_tokenize(sentence.lower()):\n",
    "        if word in word2count.keys():\n",
    "            if len(sentence.split(' ')) < 25: #assumption that long sentences not so important, makes summarizer more efficient\n",
    "                if sentence not in sent2score.keys(): #first occurence\n",
    "                    if sentence not in sent2score.keys():\n",
    "                        sent2score[sentence] = word2count[word]\n",
    "                    else:\n",
    "                        sent2score[sentence] += word2count[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2015 updates to account for differing methods of measuring ocean surface temperature measurements show a positive trend over the recent decade.': 0.04,\n",
       " ':290 This mandate was sustained in the Kyoto Protocol to the Framework Convention, :290 which entered into legal effect in 2005.': 0.4,\n",
       " ':5 At the 15th UNFCCC Conference of the Parties, held in 2009 at Copenhagen, several UNFCCC Parties produced the Copenhagen Accord.': 0.2,\n",
       " ':71 Emissions can be attributed to different regions.': 1.0,\n",
       " 'A 2015 report by Citibank concluded that transitioning to a low carbon economy would yield positive return on investments.': 0.2,\n",
       " 'A March–May 2013 survey by Pew Research Center for the People & the Press polled 39 countries about global threats.': 0.5,\n",
       " 'A climate model is a representation of the physical, chemical and biological processes that affect the climate system.': 1.0,\n",
       " 'A global 2015 Pew Research Center report showed that a median of 54% of all respondents asked consider it \"a very serious problem\".': 0.7954545454545454,\n",
       " 'According to 54% of those questioned, global warming featured top of the perceived global threats.': 0.2,\n",
       " 'According to basic physical principles, the greenhouse effect produces warming of the lower atmosphere (the troposphere), but cooling of the upper atmosphere (the stratosphere).': 0.2,\n",
       " 'According to professor Brian Hoskins, this is likely the first time CO2 levels have been this high for about 4.5 million years.': 0.2,\n",
       " 'According to work published in 2007, the concentrations of CO2 and methane had increased by 36% and 148% respectively since 1750.': 0.2,\n",
       " 'Adaptation is especially important in developing countries since those countries are predicted to bear the brunt of the effects of global warming.': 0.28,\n",
       " 'Additional disputes concern estimates of climate sensitivity, predictions of additional warming, and what the consequences of global warming will be.': 0.16,\n",
       " 'Africa is one of the most vulnerable continents to climate variability and change because of multiple existing stresses and low adaptive capacity.': 0.8,\n",
       " 'All regions are at risk of experiencing negative impacts.': 0.2,\n",
       " 'Although there are a few areas of linkage, the relationship between the two is not strong.': 0.24,\n",
       " \"Another line of evidence for the Sun's non-attributability is the differing temperature changes at different levels in the Earth's atmosphere.\": 0.24,\n",
       " 'Another prominent research topic is expanding and improving representations of the carbon cycle.': 0.24,\n",
       " 'Another significant non-fuel source of anthropogenic CO2 emissions is the calcination of limestone for clinker production, a chemical process which releases CO2.': 0.24,\n",
       " 'Anticipated effects include increasing global temperatures, rising sea levels, changing precipitation, and expansion of deserts in the subtropics.': 0.04,\n",
       " 'At least one region — the southeastern part of the United States — has experienced cooler than normal temperatures.': 0.12,\n",
       " 'Atmospheric soot directly absorbs solar radiation, which heats the atmosphere and cools the surface.': 0.4,\n",
       " 'Attributions of emissions due to land-use change are subject to considerable uncertainty.': 0.04,\n",
       " 'Because it is not a uniform phenomenon, effects can vary by region.': 0.08,\n",
       " 'CO2 emissions are continuing to rise due to the burning of fossil fuels and land-use change.': 1.0,\n",
       " 'Climate change adaptation is another policy response.': 1.0,\n",
       " 'Climate engineering (sometimes called geoengineering or climate intervention) is the deliberate modification of the climate.': 1.0,\n",
       " 'Climate models do not account for this possible feedback mechanism.': 1.0,\n",
       " 'Climate models have been used to examine the role of the Sun in recent climate change.': 1.0,\n",
       " 'Climate models produce a good match to observations of global temperature changes over the last century, but do not simulate all aspects of climate.': 1.0,\n",
       " 'Climate variability and change is projected to severely compromise agricultural production, including access to food, across Africa.': 1.0,\n",
       " 'Clouds also affect the radiation balance through cloud forcings similar to greenhouse gases.': 0.52,\n",
       " 'Clouds and their effects are especially difficult to predict.': 0.52,\n",
       " 'Coal burning was responsible for 43% of the total emissions, oil 34%, gas 18%, cement 4.9% and gas flaring 0.7%.': 0.04,\n",
       " 'Existing stresses include poverty, political conflicts, and ecosystem degradation.': 0.4,\n",
       " 'Feedbacks are an important factor in determining the sensitivity of the climate system to increased atmospheric greenhouse gas concentrations.': 0.36,\n",
       " 'Fossil fuel burning has produced about three-quarters of the increase in CO2 from human activity over the past 20 years.': 0.4,\n",
       " 'Fossil fuel reserves are abundant, and will not limit carbon emissions in the 21st century.': 0.4,\n",
       " \"From 1906 to 2005, Earth's average surface temperature rose by 7002273890000000000♠0.74±0.18 °C.\": 0.4666666666666667,\n",
       " 'Future climate change and associated impacts will differ from region to region.': 0.48,\n",
       " \"Global dimming, a gradual reduction in the amount of global direct irradiance at the Earth's surface, was observed from 1961 until at least 1990.\": 0.7954545454545454,\n",
       " 'Global oil companies have begun to acknowledge climate change exists and is caused by human activities and the burning of fossil fuels.': 0.7954545454545454,\n",
       " 'Global temperature is subject to short-term fluctuations that overlay long-term trends and can temporarily mask them.': 0.7954545454545454,\n",
       " 'Global warming refers to global averages.': 0.7954545454545454,\n",
       " \"However, the IPCC's projections do not reflect the full range of uncertainty.\": 0.3333333333333333,\n",
       " 'If solar variations were responsible for the observed warming, warming of both the troposphere and the stratosphere would be expected.': 0.4,\n",
       " \"Improving the models' representation of clouds is therefore an important topic in current research.\": 0.5,\n",
       " 'In 1986 and November 1987, NASA climate scientist James Hansen gave testimony to Congress on global warming.': 0.2,\n",
       " \"In May 2013, it was reported that readings for CO2 taken at the world's primary benchmark site in Mauna Loa surpassed 400 ppm.\": 0.36,\n",
       " \"In addition to their direct effect by scattering and absorbing solar radiation, aerosols have indirect effects on the Earth's radiation budget.\": 0.16666666666666666,\n",
       " 'In limiting warming at below 2 °C, more stringent emission reductions in the near-term would allow for less rapid reductions after 2030.': 0.8,\n",
       " 'In most scenarios, emissions continue to rise over the century, while in a few, emissions are reduced.': 0.24,\n",
       " 'In ratifying the Kyoto Protocol, most developed countries accepted legally binding commitments to limit their emissions.': 0.2,\n",
       " 'In small islands and mega deltas, inundation as a result of sea level rise is expected to threaten vital infrastructure and human settlements.': 0.6,\n",
       " 'In the 1950s, research suggested increasing temperatures, and a 1952 newspaper reported \"climate change\".': 0.44,\n",
       " 'In the 1960s, the warming effect of carbon dioxide gas became increasingly convincing.': 0.8636363636363636,\n",
       " 'In the United States from about 1990 onwards, American conservative think tanks had begun challenging the legitimacy of global warming as a social problem.': 0.28,\n",
       " 'In the late 19th century, scientists first argued that human emissions of greenhouse gases could change the climate.': 0.6666666666666666,\n",
       " 'In the past one hundred years, average arctic temperatures have been increasing at almost twice the rate of the rest of the world.': 0.28,\n",
       " 'Indirect effects are most noticeable in marine stratiform clouds, and have very little radiative effect on convective clouds.': 0.5,\n",
       " 'Indirect effects of aerosols represent the largest uncertainty in radiative forcing.': 0.5,\n",
       " 'Instead the models predict how greenhouse gases will interact with radiative transfer and other physical processes.': 0.16666666666666666,\n",
       " 'It also recognized the need to consider strengthening the goal to a global average rise of 7002274649999999999♠1.5 °C.': 0.11363636363636363,\n",
       " 'It has been investigated as a possible response to global warming, e.g.': 0.12,\n",
       " 'Latin America had the biggest rise in concern: 73% said global warming was a serious threat to their families.': 0.5,\n",
       " 'Less direct geological evidence indicates that CO2 values higher than this were last seen about 20 million years ago.': 0.2,\n",
       " 'Low-latitude, less developed areas face the greatest risk.': 0.2,\n",
       " 'Many integrated models are unable to meet the 2 °C target if pessimistic assumptions are made about the availability of mitigation technologies.': 0.13333333333333333,\n",
       " 'Many risks are expected to increase with higher magnitudes of global warming.': 0.13333333333333333,\n",
       " 'Models are unable to reproduce the rapid warming observed in recent decades when only taking into account variations in solar output and volcanic activity.': 1.0,\n",
       " 'Monthly global CO2 concentrations exceeded 400 ppm in March 2015, probably for the first time in several million years.': 0.04,\n",
       " 'More research is needed to understand the role of clouds and carbon cycle feedbacks in climate projections.': 0.44,\n",
       " 'Most countries are parties to the United Nations Framework Convention on Climate Change (UNFCCC), whose ultimate objective is to prevent dangerous anthropogenic climate change.': 0.72,\n",
       " 'Most countries in the world are parties to the United Nations Framework Convention on Climate Change (UNFCCC).': 0.72,\n",
       " 'Multiple lines of scientific evidence show that the climate system is warming.': 0.1111111111111111,\n",
       " 'National science academies have called on world leaders for policies to cut global emissions.': 0.24,\n",
       " 'No scientific body of national or international standing disagrees with this view.': 0.3111111111111111,\n",
       " 'Not all effects of global warming are accurately predicted by the climate models used by the IPCC.': 0.5333333333333333,\n",
       " 'Observed Arctic shrinkage has been faster than that predicted.': 0.26666666666666666,\n",
       " 'Orbital cycles favorable for glaciation are not expected within the next 50,000 years.': 0.16666666666666666,\n",
       " 'Other factors being equal, a higher climate sensitivity means that more warming will occur for a given increase in greenhouse gas forcing.': 0.16666666666666666,\n",
       " 'Planned adaptation is already occurring on a limited basis.': 0.4,\n",
       " 'Polling groups began to track opinions on the subject, at first mainly in the United States.': 0.6666666666666666,\n",
       " 'Positive feedbacks increase the response of the climate system to an initial forcing, while negative feedbacks reduce it.': 0.12,\n",
       " 'Precipitation increased proportionally to atmospheric humidity, and hence significantly faster than global climate models predict.': 0.16,\n",
       " 'Public attention increased over the summer, and global warming became the dominant popular term, commonly used both by the press and in public discourse.': 0.28,\n",
       " 'Public reactions to global warming and concern about its effects are also increasing.': 0.28,\n",
       " 'Radiative forcing due to aerosols is temporally limited due to the processes that remove aerosols from the atmosphere.': 0.28,\n",
       " 'Reduced stratospheric ozone has had a slight cooling influence on surface temperatures, while increased tropospheric ozone has had a somewhat larger warming effect.': 0.08,\n",
       " 'Research during this period has been summarized in the Assessment Reports by the Intergovernmental Panel on Climate Change.': 0.44,\n",
       " \"Results from models can also vary due to different greenhouse gas inputs and the model's climate sensitivity.\": 0.16666666666666666,\n",
       " 'Significant regional differences exist, with Americans and Chinese (whose economies are responsible for the greatest annual CO2 emissions) among the least concerned.': 0.2,\n",
       " 'Since 1978, solar irradiance has been measured by satellites.': 0.4666666666666667,\n",
       " 'Since 1990, sea level has also risen considerably faster than models predicted it would.': 0.4666666666666667,\n",
       " 'Since the 1990s, scientific research on climate change has included multiple disciplines and has expanded.': 0.4666666666666667,\n",
       " 'Sixteen of the seventeen warmest years on record have occurred since 2000.': 0.04,\n",
       " 'Solid and liquid particles known as aerosols, produced by volcanoes and human-made pollutants, are thought to be the main cause of this dimming.': 0.1,\n",
       " 'Some climatologists have criticized the attention that the popular press gives to \"warmest year\" statistics.': 0.04,\n",
       " 'Some of this surface warming will be driven by past natural forcings which are still seeking equilibrium in the climate system.': 0.6153846153846154,\n",
       " 'Some people dispute aspects of climate change science.': 0.8,\n",
       " \"Soot may either cool or warm Earth's climate system, depending on whether it is airborne or deposited.\": 0.5,\n",
       " 'Such models are based on scientific disciplines such as fluid dynamics and thermodynamics as well as physical processes such as radiative transfer.': 1.0,\n",
       " 'Sulfate aerosols act as cloud condensation nuclei and thus lead to clouds that have more and smaller cloud droplets.': 0.16666666666666666,\n",
       " 'Techniques under research fall generally into the categories solar radiation management and carbon dioxide removal, although various other schemes have been suggested.': 0.2,\n",
       " 'The 16th Conference of the Parties (COP16) was held at Cancún in 2010.': 0.4,\n",
       " 'The Framework Convention was agreed on in 1992, but global emissions have risen since then.': 0.16,\n",
       " 'The IPCC projections previously mentioned span the \"likely\" range (greater than 66% probability, based on expert judgement) for the selected emissions scenarios.': 0.2222222222222222,\n",
       " 'The adaptation may be planned, either in reaction to or anticipation of global warming, or spontaneous, i.e., without government intervention.': 0.28,\n",
       " 'The barriers, limits, and costs of future adaptation are not fully understood.': 0.2,\n",
       " 'The climate system includes a range of feedbacks, which alter the response of the system to changes in external forcings.': 1.0,\n",
       " 'The environmental effects of global warming are broad and far reaching.': 0.5,\n",
       " 'The future social impacts of climate change will be uneven across the world.': 0.48,\n",
       " 'The global warming problem came to international public attention in the late 1980s.': 0.7954545454545454,\n",
       " 'The largest human influence has been the emission of greenhouse gases such as carbon dioxide, methane, and nitrous oxide.': 0.044444444444444446,\n",
       " 'The lower end of the \"likely\" range appears to be better constrained than the upper end.': 0.2,\n",
       " \"The main negative feedback is the energy the Earth's surface radiates into space as infrared radiation.\": 0.16,\n",
       " 'The models do not assume the climate will warm due to increasing levels of greenhouse gases.': 1.0,\n",
       " 'The phenomenon is sometimes called \"anthropogenic global warming\" or \"anthropogenic climate change\" in view of the dominant role of human activity as its cause.': 0.08888888888888889,\n",
       " 'The phrase began to come into common use, and in 1976 Mikhail Budyko\\'s statement that \"a global warming up has started\" was widely reported.': 1.0,\n",
       " 'The physical realism of models is tested by examining their ability to simulate contemporary or past climates.': 0.8333333333333334,\n",
       " 'The probability that these changes could have occurred by chance is virtually zero.': 0.12,\n",
       " 'The rate of warming almost doubled in the last half of that period (7002273279999999999♠0.13±0.03 °C per decade, against 7002273219999999999♠0.07±0.02 °C per decade).': 0.08,\n",
       " 'The rest has melted ice and warmed the continents and the atmosphere.': 0.12,\n",
       " 'The rest of this increase is caused mostly by changes in land-use, particularly deforestation.': 0.12,\n",
       " 'The tilt of the Earth’s axis and the shape of its orbit around the Sun vary slowly over tens of thousands of years.': 0.16666666666666666,\n",
       " 'The ultimate objective of the Convention is to prevent dangerous human interference of the climate system.': 0.08,\n",
       " 'The warming evident in the instrumental temperature record is consistent with a wide range of observations, as documented by many independent scientific groups.': 0.8636363636363636,\n",
       " 'These clouds reflect solar radiation more efficiently than clouds with fewer and larger droplets, a phenomenon known as the Twomey effect.': 0.52,\n",
       " 'These first-round commitments expired in 2012.': 0.4,\n",
       " 'These forcings are \"external\" to the climate system, but not necessarily external to Earth.': 0.36,\n",
       " 'They challenged the scientific evidence, argued that global warming would have benefits, and asserted that proposed solutions would do more harm than good.': 0.6666666666666666,\n",
       " 'They exert a cooling effect by increasing the reflection of incoming sunlight.': 0.16666666666666666,\n",
       " \"This changes climate by changing the seasonal and latitudinal distribution of incoming solar energy at Earth's surface.\": 0.5333333333333333,\n",
       " 'This is 90–250% above the concentration in the year 1750.': 0.2,\n",
       " 'Uncertainty over the effect of feedbacks is a major reason why different climate models project different magnitudes of warming for a given forcing scenario.': 0.2,\n",
       " 'Warming or cooling is thus a result, not an assumption, of the models.': 0.8636363636363636,\n",
       " 'When deposited, especially on glaciers or on ice in arctic regions, the lower surface albedo can also directly heat the surface.': 0.3333333333333333,\n",
       " 'While record-breaking years attract considerable public interest, individual years are less significant than the overall trend.': 0.4444444444444444,\n",
       " \"[d] Without the Earth's atmosphere, the Earth's average temperature would be well below the freezing temperature of water.\": 0.12,\n",
       " 'by NASA and the Royal Society.': 0.2}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='18.5'></a>\n",
    "### Getting the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "CO2 emissions are continuing to rise due to the burning of fossil fuels and land-use change.\n",
      ":71 Emissions can be attributed to different regions.\n",
      "Climate models have been used to examine the role of the Sun in recent climate change.\n",
      "Models are unable to reproduce the rapid warming observed in recent decades when only taking into account variations in solar output and volcanic activity.\n",
      "The climate system includes a range of feedbacks, which alter the response of the system to changes in external forcings.\n"
     ]
    }
   ],
   "source": [
    "# Gettings best 5 lines             \n",
    "best_sentences = heapq.nlargest(5, sent2score, key=sent2score.get)\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "for sentence in best_sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='19'></a>\n",
    "## Word2Vec\n",
    "\n",
    "BOW, TFIDF -Problems\n",
    "\n",
    "-semantic information of the words is not stored. even in TF-IDF model we only give more importance to the uncommon words\n",
    "-there's a chance of overfitting the model. Overfitting a scenario when model performs very well with training data, but fails miserably when applied to any new dataset\n",
    "\n",
    "Word2Vec Model\n",
    "-in this model, each word is represented as vector of 32 or more dimension instead of a single number\n",
    "-relation between different words is preserved\n",
    "\n",
    "Steps to build the model\n",
    "-scrape through a huge dataset like the whole Wikipedia\n",
    "-create a matrix with all the unique words in the dataset. The matrix represents the occurrence relation between the words\n",
    "-split the matrix into two thin matrices\n",
    "-we have the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec:**\n",
    "* [ Importing the data](#19.0)\n",
    "* [Preprocessing the data](#19.1)\n",
    "* [Preparing the data](#19.2)\n",
    "* [Training the Word2Vec Model](#19.3)\n",
    "* [Testing model performance](#19.4)\n",
    "* [Improving Performance](#19.5)\n",
    "* [Exploring Pre-trained models](#19.6)\n",
    "* [](#19.7)\n",
    "* [](#19.8)\n",
    "* [](#19.9)\n",
    "* [](#19.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Y\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import urllib\n",
    "import bs4 as bs4\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='19.0'></a>\n",
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = urllib.request.urlopen('http://en.wikipedia.org/wiki/Global_warming').read()\n",
    "soup = bs4.BeautifulSoup(source,'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='19.1'></a>\n",
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "\n",
    "for paragraph in soup.find_all('p'):\n",
    "    text += paragraph.text\n",
    "\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ',text) #get rid of [#]\n",
    "text = re.sub(r'\\s+', ' ', text) #get rid of spaces\n",
    "text = text.lower() #make all lower case\n",
    "text = re.sub(r'\\W',' ',text) #remove all non-word char\n",
    "text = re.sub(r'\\d',' ',text) #remove digits\n",
    "text = re.sub(r'\\s+',' ',text) #remove extra spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='19.2'></a>\n",
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences] #tokenize the one big sentence into words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='19.3'></a>\n",
    "### Training the Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences,min_count=1) #ignore all words with min_count lower than 1; consider all the words\n",
    "\n",
    "words = model.wv.vocab #dictionary of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='19.4'></a>\n",
    "### Testing model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector = model.wv['global'] #what is the vector for the word global\n",
    "\n",
    "similar = model.wv.most_similar('warming')\n",
    "similar = model.wv.most_similar('global')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='19.5'></a>\n",
    "### Improving Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "\n",
    "for paragraph in soup.find_all('p'):\n",
    "    text += paragraph.text\n",
    "\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ',text) #get rid of [#]\n",
    "text = re.sub(r'\\s+', ' ', text) #get rid of spaces\n",
    "text = text.lower() #make all lower case\n",
    "text = re.sub(r'[@#\\$%&\\*\\(\\)\\<\\>\\?\\'\\\":;\\]\\[-]]', ' ', text)\n",
    "text = re.sub(r'\\d',' ',text) #remove digits\n",
    "text = re.sub(r'\\s+',' ',text) #remove extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "sentences = [nltk.word_tokenize(sentence) for sentence in sentences] #tokenize the one big sentence into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english') \n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences,min_count=1) #ignore all words with min_count lower than 1; consider all the words\n",
    "\n",
    "words = model.wv.vocab #dictionary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector = model.wv['global']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similar_global = model.wv.most_similar('global')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 0.7416222095489502),\n",
       " ('(', 0.6164489388465881),\n",
       " ('surface', 0.5980542898178101),\n",
       " ('clouds', 0.5944184064865112),\n",
       " ('%', 0.5892006754875183),\n",
       " ('warming', 0.577069878578186),\n",
       " ('increase', 0.5701586604118347),\n",
       " ('human', 0.5576974749565125),\n",
       " ('change', 0.5538814067840576),\n",
       " ('effects', 0.5491930246353149)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "similar_warming = model.wv.most_similar('warming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 0.6895397901535034),\n",
       " ('.', 0.5893706679344177),\n",
       " ('global', 0.577069878578186),\n",
       " ('emissions', 0.5526173114776611),\n",
       " ('increase', 0.5459699034690857),\n",
       " ('greenhouse', 0.525496244430542),\n",
       " ('national', 0.5117422342300415),\n",
       " (':', 0.5103501081466675),\n",
       " ('climate', 0.5052606463432312),\n",
       " ('human', 0.5025408267974854)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_warming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Install gensim - pip install gensim\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='19.6'></a>\n",
    "### Exploring Pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Y\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('kings', 0.7138045430183411),\n",
       " ('queen', 0.6510956883430481),\n",
       " ('monarch', 0.6413194537162781),\n",
       " ('crown_prince', 0.6204219460487366),\n",
       " ('prince', 0.6159993410110474),\n",
       " ('sultan', 0.5864823460578918),\n",
       " ('ruler', 0.5797567367553711),\n",
       " ('princes', 0.5646552443504333),\n",
       " ('Prince_Paras', 0.5432944297790527),\n",
       " ('throne', 0.5422105193138123)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "\n",
    "model.wv.most_similar('king') #queen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Y\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431607246399),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.5181134343147278),\n",
       " ('sultan', 0.5098593235015869),\n",
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['king','woman'], negative= ['man']) #king+woman-man=queen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
